{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "layers = tf.contrib.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = os.environ['DATA_GENERATOR']\n",
    "sys.path.append(data_generator)\n",
    "import get_data\n",
    "from visualization import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "# sess = tf.Session()\n",
    "# x = sess.run(images)\n",
    "# print(x.shape)\n",
    "# print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.batch_size = 128\n",
    "        with self.graph.as_default():\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "            self.visualizer = Visualizer(exp_name='AE_mnist', row=8, col=8)\n",
    "            self.generated_images = self._build_graph(self.images, 5, reuse=tf.AUTO_REUSE, training=True)\n",
    "            self.loss = self._loss_function(self.images, self.generated_images)\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=0.0001) \\\n",
    "                           .minimize(self.loss)\n",
    "            initializer = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(initializer)\n",
    "    def train(self, iteration):\n",
    "        for i in range(iteration+1):\n",
    "            loss, _, np_real_images, np_generated_images = self.sess.run(\n",
    "                    [self.loss, self.solver, self.images, self.generated_images])\n",
    "            if i % 10 == 0:\n",
    "                print(\"iterator {} : loss {} \".format(i, loss))\n",
    "            if i % 1000 == 0:\n",
    "                self.visualize(np_real_images, np_generated_images, i)\n",
    "            \n",
    "    def visualize(self, images, generated_images, i):\n",
    "        visual_imgs = np.concatenate( (images[:8*4], generated_images[:8*4]), axis = 0 )\n",
    "        visual_imgs = 255*(visual_imgs/2 + 0.5)\n",
    "        visual_imgs = visual_imgs.astype(int)\n",
    "        self.visualizer.draw_imgs(visual_imgs)\n",
    "        self.visualizer.save_fig(name=\"AE_mnist_iter_{}\".format(str(i)))\n",
    "        \n",
    "    def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "        net = layers.conv2d(input, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 128, weights_regularizer=regularizer)\n",
    "        latent_var = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "        \n",
    "        net = layers.fully_connected(latent_var, 128, weights_regularizer=regularizer)\n",
    "        net = layers.fully_connected(latent_var, 7*7*32, weights_regularizer=regularizer)\n",
    "        net = tf.reshape(shape=[-1, 7, 7, 32], tensor=net)\n",
    "        net = layers.conv2d_transpose(net, 32, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 32, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "        return net\n",
    "    def _loss_function(self, _real_images, _generated_images):\n",
    "        recon_loss = tf.reduce_mean(tf.square(_real_images - _generated_images))\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 0 : loss 0.9057450294494629 \n",
      "iterator 10 : loss 0.8897145390510559 \n",
      "iterator 20 : loss 0.8583968877792358 \n",
      "iterator 30 : loss 0.7694113850593567 \n",
      "iterator 40 : loss 0.5553624629974365 \n",
      "iterator 50 : loss 0.3581501841545105 \n",
      "iterator 60 : loss 0.3188525140285492 \n",
      "iterator 70 : loss 0.3023625612258911 \n",
      "iterator 80 : loss 0.3411181569099426 \n",
      "iterator 90 : loss 0.33557844161987305 \n",
      "iterator 100 : loss 0.3709307312965393 \n",
      "iterator 110 : loss 0.3373946249485016 \n",
      "iterator 120 : loss 0.2920726537704468 \n",
      "iterator 130 : loss 0.3214159309864044 \n",
      "iterator 140 : loss 0.2908421456813812 \n",
      "iterator 150 : loss 0.335613876581192 \n",
      "iterator 160 : loss 0.3363533020019531 \n",
      "iterator 170 : loss 0.3296518921852112 \n",
      "iterator 180 : loss 0.3415021598339081 \n",
      "iterator 190 : loss 0.3214062452316284 \n",
      "iterator 200 : loss 0.3043321967124939 \n",
      "iterator 210 : loss 0.3417326509952545 \n",
      "iterator 220 : loss 0.36054736375808716 \n",
      "iterator 230 : loss 0.26960763335227966 \n",
      "iterator 240 : loss 0.22728335857391357 \n",
      "iterator 250 : loss 0.22352160513401031 \n",
      "iterator 260 : loss 0.2030627429485321 \n",
      "iterator 270 : loss 0.19681553542613983 \n",
      "iterator 280 : loss 0.1847812533378601 \n",
      "iterator 290 : loss 0.18891288340091705 \n",
      "iterator 300 : loss 0.18092405796051025 \n",
      "iterator 310 : loss 0.18092218041419983 \n",
      "iterator 320 : loss 0.18440468609333038 \n",
      "iterator 330 : loss 0.19635790586471558 \n",
      "iterator 340 : loss 0.1713929921388626 \n",
      "iterator 350 : loss 0.17613856494426727 \n",
      "iterator 360 : loss 0.18105818331241608 \n",
      "iterator 370 : loss 0.17974503338336945 \n",
      "iterator 380 : loss 0.1744764745235443 \n",
      "iterator 390 : loss 0.17557168006896973 \n",
      "iterator 400 : loss 0.18554089963436127 \n",
      "iterator 410 : loss 0.16858814656734467 \n",
      "iterator 420 : loss 0.17505016922950745 \n",
      "iterator 430 : loss 0.1813584864139557 \n",
      "iterator 440 : loss 0.1811465620994568 \n",
      "iterator 450 : loss 0.1723185032606125 \n",
      "iterator 460 : loss 0.18349099159240723 \n",
      "iterator 470 : loss 0.17194798588752747 \n",
      "iterator 480 : loss 0.17976193130016327 \n",
      "iterator 490 : loss 0.16673463582992554 \n",
      "iterator 500 : loss 0.18009963631629944 \n",
      "iterator 510 : loss 0.1654481440782547 \n",
      "iterator 520 : loss 0.1683308482170105 \n",
      "iterator 530 : loss 0.17242443561553955 \n",
      "iterator 540 : loss 0.17700189352035522 \n",
      "iterator 550 : loss 0.17274411022663116 \n",
      "iterator 560 : loss 0.16955365240573883 \n",
      "iterator 570 : loss 0.1757449358701706 \n",
      "iterator 580 : loss 0.16441982984542847 \n",
      "iterator 590 : loss 0.1553407907485962 \n",
      "iterator 600 : loss 0.15367341041564941 \n",
      "iterator 610 : loss 0.15086977183818817 \n",
      "iterator 620 : loss 0.15894879400730133 \n",
      "iterator 630 : loss 0.14059485495090485 \n",
      "iterator 640 : loss 0.1455368846654892 \n",
      "iterator 650 : loss 0.14311574399471283 \n",
      "iterator 660 : loss 0.13335534930229187 \n",
      "iterator 670 : loss 0.1396864652633667 \n",
      "iterator 680 : loss 0.14100588858127594 \n",
      "iterator 690 : loss 0.1418682336807251 \n",
      "iterator 700 : loss 0.13021957874298096 \n",
      "iterator 710 : loss 0.12366462498903275 \n",
      "iterator 720 : loss 0.13139097392559052 \n",
      "iterator 730 : loss 0.11808034032583237 \n",
      "iterator 740 : loss 0.12644457817077637 \n",
      "iterator 750 : loss 0.1332646906375885 \n",
      "iterator 760 : loss 0.1325145810842514 \n",
      "iterator 770 : loss 0.13083559274673462 \n",
      "iterator 780 : loss 0.12479556351900101 \n",
      "iterator 790 : loss 0.12275400012731552 \n",
      "iterator 800 : loss 0.12522351741790771 \n",
      "iterator 810 : loss 0.12445919215679169 \n",
      "iterator 820 : loss 0.1296120136976242 \n",
      "iterator 830 : loss 0.13370166718959808 \n",
      "iterator 840 : loss 0.1282559633255005 \n",
      "iterator 850 : loss 0.11903469264507294 \n",
      "iterator 860 : loss 0.12778174877166748 \n",
      "iterator 870 : loss 0.12310126423835754 \n",
      "iterator 880 : loss 0.12121453881263733 \n",
      "iterator 890 : loss 0.12395521998405457 \n",
      "iterator 900 : loss 0.12309518456459045 \n",
      "iterator 910 : loss 0.12009166181087494 \n",
      "iterator 920 : loss 0.11597098410129547 \n",
      "iterator 930 : loss 0.13234853744506836 \n",
      "iterator 940 : loss 0.1147746816277504 \n",
      "iterator 950 : loss 0.1194433867931366 \n",
      "iterator 960 : loss 0.12143132835626602 \n",
      "iterator 970 : loss 0.1295056939125061 \n",
      "iterator 980 : loss 0.12398337572813034 \n",
      "iterator 990 : loss 0.1138453260064125 \n",
      "iterator 1000 : loss 0.11896590143442154 \n",
      "iterator 1010 : loss 0.1279410421848297 \n",
      "iterator 1020 : loss 0.11256357282400131 \n",
      "iterator 1030 : loss 0.1215578019618988 \n",
      "iterator 1040 : loss 0.11657177656888962 \n",
      "iterator 1050 : loss 0.12211482226848602 \n",
      "iterator 1060 : loss 0.11614495515823364 \n",
      "iterator 1070 : loss 0.12638044357299805 \n",
      "iterator 1080 : loss 0.10713502019643784 \n",
      "iterator 1090 : loss 0.12578682601451874 \n",
      "iterator 1100 : loss 0.11750071495771408 \n",
      "iterator 1110 : loss 0.11380735039710999 \n",
      "iterator 1120 : loss 0.11943238973617554 \n",
      "iterator 1130 : loss 0.10846535861492157 \n",
      "iterator 1140 : loss 0.1061880961060524 \n",
      "iterator 1150 : loss 0.11648403853178024 \n",
      "iterator 1160 : loss 0.12423922121524811 \n",
      "iterator 1170 : loss 0.11813399195671082 \n",
      "iterator 1180 : loss 0.1070498451590538 \n",
      "iterator 1190 : loss 0.11397447437047958 \n",
      "iterator 1200 : loss 0.11192992329597473 \n",
      "iterator 1210 : loss 0.11879368871450424 \n",
      "iterator 1220 : loss 0.11552197486162186 \n",
      "iterator 1230 : loss 0.11759427189826965 \n",
      "iterator 1240 : loss 0.10708186030387878 \n",
      "iterator 1250 : loss 0.11924349516630173 \n",
      "iterator 1260 : loss 0.1116996482014656 \n",
      "iterator 1270 : loss 0.10742552578449249 \n",
      "iterator 1280 : loss 0.11417277902364731 \n",
      "iterator 1290 : loss 0.10548229515552521 \n",
      "iterator 1300 : loss 0.11479225009679794 \n",
      "iterator 1310 : loss 0.12027467787265778 \n",
      "iterator 1320 : loss 0.11432948708534241 \n",
      "iterator 1330 : loss 0.10975434631109238 \n",
      "iterator 1340 : loss 0.11122971028089523 \n",
      "iterator 1350 : loss 0.10635162144899368 \n",
      "iterator 1360 : loss 0.10585053265094757 \n",
      "iterator 1370 : loss 0.11340601742267609 \n",
      "iterator 1380 : loss 0.10246183723211288 \n",
      "iterator 1390 : loss 0.10957517474889755 \n",
      "iterator 1400 : loss 0.112799271941185 \n",
      "iterator 1410 : loss 0.11142788827419281 \n",
      "iterator 1420 : loss 0.11209029704332352 \n",
      "iterator 1430 : loss 0.10391470789909363 \n",
      "iterator 1440 : loss 0.11425291001796722 \n",
      "iterator 1450 : loss 0.11430763453245163 \n",
      "iterator 1460 : loss 0.10083233565092087 \n",
      "iterator 1470 : loss 0.1218026727437973 \n",
      "iterator 1480 : loss 0.10919790714979172 \n",
      "iterator 1490 : loss 0.10824229568243027 \n",
      "iterator 1500 : loss 0.11429595947265625 \n",
      "iterator 1510 : loss 0.10808593034744263 \n",
      "iterator 1520 : loss 0.11124766618013382 \n",
      "iterator 1530 : loss 0.10765893012285233 \n",
      "iterator 1540 : loss 0.11302575469017029 \n",
      "iterator 1550 : loss 0.10193853825330734 \n",
      "iterator 1560 : loss 0.11996403336524963 \n",
      "iterator 1570 : loss 0.10724464058876038 \n",
      "iterator 1580 : loss 0.11459378153085709 \n",
      "iterator 1590 : loss 0.10884596407413483 \n",
      "iterator 1600 : loss 0.10656809061765671 \n",
      "iterator 1610 : loss 0.1042863205075264 \n",
      "iterator 1620 : loss 0.10731349885463715 \n",
      "iterator 1630 : loss 0.1020585298538208 \n",
      "iterator 1640 : loss 0.11090951412916183 \n",
      "iterator 1650 : loss 0.10170255601406097 \n",
      "iterator 1660 : loss 0.10947421938180923 \n",
      "iterator 1670 : loss 0.10215180367231369 \n",
      "iterator 1680 : loss 0.1102202832698822 \n",
      "iterator 1690 : loss 0.10905449092388153 \n",
      "iterator 1700 : loss 0.10698423534631729 \n",
      "iterator 1710 : loss 0.10845880955457687 \n",
      "iterator 1720 : loss 0.11099928617477417 \n",
      "iterator 1730 : loss 0.10446427762508392 \n",
      "iterator 1740 : loss 0.11722484976053238 \n",
      "iterator 1750 : loss 0.10651002824306488 \n",
      "iterator 1760 : loss 0.11041965335607529 \n",
      "iterator 1770 : loss 0.10405615717172623 \n",
      "iterator 1780 : loss 0.1088457778096199 \n",
      "iterator 1790 : loss 0.11379867047071457 \n",
      "iterator 1800 : loss 0.10757681727409363 \n",
      "iterator 1810 : loss 0.09327331930398941 \n",
      "iterator 1820 : loss 0.09707968682050705 \n",
      "iterator 1830 : loss 0.10730382055044174 \n",
      "iterator 1840 : loss 0.1055508404970169 \n",
      "iterator 1850 : loss 0.09821809828281403 \n",
      "iterator 1860 : loss 0.09925445914268494 \n",
      "iterator 1870 : loss 0.1098642572760582 \n",
      "iterator 1880 : loss 0.09913076460361481 \n",
      "iterator 1890 : loss 0.10187584906816483 \n",
      "iterator 1900 : loss 0.1061425730586052 \n",
      "iterator 1910 : loss 0.10821262747049332 \n",
      "iterator 1920 : loss 0.10072415322065353 \n",
      "iterator 1930 : loss 0.09631127119064331 \n",
      "iterator 1940 : loss 0.10825156420469284 \n",
      "iterator 1950 : loss 0.10091457515954971 \n",
      "iterator 1960 : loss 0.09953724592924118 \n",
      "iterator 1970 : loss 0.1017734557390213 \n",
      "iterator 1980 : loss 0.1080905944108963 \n",
      "iterator 1990 : loss 0.0976676270365715 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 2000 : loss 0.09822224825620651 \n",
      "iterator 2010 : loss 0.10762204974889755 \n",
      "iterator 2020 : loss 0.09526433795690536 \n",
      "iterator 2030 : loss 0.11021547019481659 \n",
      "iterator 2040 : loss 0.109731525182724 \n",
      "iterator 2050 : loss 0.10872303694486618 \n",
      "iterator 2060 : loss 0.09955958276987076 \n",
      "iterator 2070 : loss 0.10014385730028152 \n",
      "iterator 2080 : loss 0.1015157699584961 \n",
      "iterator 2090 : loss 0.10411449521780014 \n",
      "iterator 2100 : loss 0.10731969773769379 \n",
      "iterator 2110 : loss 0.10357426851987839 \n",
      "iterator 2120 : loss 0.10235095769166946 \n",
      "iterator 2130 : loss 0.10798228532075882 \n",
      "iterator 2140 : loss 0.09679704904556274 \n",
      "iterator 2150 : loss 0.0955444723367691 \n",
      "iterator 2160 : loss 0.10341241210699081 \n",
      "iterator 2170 : loss 0.10686622560024261 \n",
      "iterator 2180 : loss 0.09081653505563736 \n",
      "iterator 2190 : loss 0.10846009850502014 \n",
      "iterator 2200 : loss 0.09979763627052307 \n",
      "iterator 2210 : loss 0.09817874431610107 \n",
      "iterator 2220 : loss 0.10004978626966476 \n",
      "iterator 2230 : loss 0.10338880866765976 \n",
      "iterator 2240 : loss 0.10004573315382004 \n",
      "iterator 2250 : loss 0.10642056912183762 \n",
      "iterator 2260 : loss 0.11272893846035004 \n",
      "iterator 2270 : loss 0.10050681978464127 \n",
      "iterator 2280 : loss 0.09118438512086868 \n",
      "iterator 2290 : loss 0.0982077568769455 \n",
      "iterator 2300 : loss 0.10387910902500153 \n",
      "iterator 2310 : loss 0.10048411041498184 \n",
      "iterator 2320 : loss 0.10029000788927078 \n",
      "iterator 2330 : loss 0.09165944159030914 \n",
      "iterator 2340 : loss 0.10623112320899963 \n",
      "iterator 2350 : loss 0.10417700558900833 \n",
      "iterator 2360 : loss 0.09358581155538559 \n",
      "iterator 2370 : loss 0.0965292826294899 \n",
      "iterator 2380 : loss 0.10629495978355408 \n",
      "iterator 2390 : loss 0.09718373417854309 \n",
      "iterator 2400 : loss 0.09095124155282974 \n",
      "iterator 2410 : loss 0.09820792078971863 \n",
      "iterator 2420 : loss 0.10262728482484818 \n",
      "iterator 2430 : loss 0.09709758311510086 \n",
      "iterator 2440 : loss 0.10557318478822708 \n",
      "iterator 2450 : loss 0.11306155472993851 \n",
      "iterator 2460 : loss 0.10085758566856384 \n",
      "iterator 2470 : loss 0.09890002757310867 \n",
      "iterator 2480 : loss 0.09541136026382446 \n",
      "iterator 2490 : loss 0.10160010308027267 \n",
      "iterator 2500 : loss 0.10062017291784286 \n",
      "iterator 2510 : loss 0.10388593375682831 \n",
      "iterator 2520 : loss 0.115364208817482 \n",
      "iterator 2530 : loss 0.09671018272638321 \n",
      "iterator 2540 : loss 0.10394923388957977 \n",
      "iterator 2550 : loss 0.09647223353385925 \n",
      "iterator 2560 : loss 0.10771512240171432 \n",
      "iterator 2570 : loss 0.10328549891710281 \n",
      "iterator 2580 : loss 0.10555372387170792 \n",
      "iterator 2590 : loss 0.09922803938388824 \n",
      "iterator 2600 : loss 0.10232016444206238 \n",
      "iterator 2610 : loss 0.10533622652292252 \n",
      "iterator 2620 : loss 0.10115287452936172 \n",
      "iterator 2630 : loss 0.10114573687314987 \n",
      "iterator 2640 : loss 0.1062135323882103 \n",
      "iterator 2650 : loss 0.10513313114643097 \n",
      "iterator 2660 : loss 0.09059921652078629 \n",
      "iterator 2670 : loss 0.10300508141517639 \n",
      "iterator 2680 : loss 0.10340219736099243 \n",
      "iterator 2690 : loss 0.09324829280376434 \n",
      "iterator 2700 : loss 0.10883355885744095 \n",
      "iterator 2710 : loss 0.10010121762752533 \n",
      "iterator 2720 : loss 0.10161793231964111 \n",
      "iterator 2730 : loss 0.10592909902334213 \n",
      "iterator 2740 : loss 0.09876037389039993 \n",
      "iterator 2750 : loss 0.09419478476047516 \n",
      "iterator 2760 : loss 0.09601587802171707 \n",
      "iterator 2770 : loss 0.09737886488437653 \n",
      "iterator 2780 : loss 0.09714816510677338 \n",
      "iterator 2790 : loss 0.0982678085565567 \n",
      "iterator 2800 : loss 0.08746174722909927 \n",
      "iterator 2810 : loss 0.10229545831680298 \n",
      "iterator 2820 : loss 0.09978651255369186 \n",
      "iterator 2830 : loss 0.09678462892770767 \n",
      "iterator 2840 : loss 0.09877557307481766 \n",
      "iterator 2850 : loss 0.10068533569574356 \n",
      "iterator 2860 : loss 0.09285997599363327 \n",
      "iterator 2870 : loss 0.1011076495051384 \n",
      "iterator 2880 : loss 0.09480758756399155 \n",
      "iterator 2890 : loss 0.0992794930934906 \n",
      "iterator 2900 : loss 0.0895300954580307 \n",
      "iterator 2910 : loss 0.10106481611728668 \n",
      "iterator 2920 : loss 0.10548768937587738 \n",
      "iterator 2930 : loss 0.10431305319070816 \n",
      "iterator 2940 : loss 0.09866410493850708 \n",
      "iterator 2950 : loss 0.0930127426981926 \n",
      "iterator 2960 : loss 0.09709247201681137 \n",
      "iterator 2970 : loss 0.0904201865196228 \n",
      "iterator 2980 : loss 0.0991981104016304 \n",
      "iterator 2990 : loss 0.11258506029844284 \n",
      "iterator 3000 : loss 0.09233163297176361 \n",
      "iterator 3010 : loss 0.09536588191986084 \n",
      "iterator 3020 : loss 0.09551659226417542 \n",
      "iterator 3030 : loss 0.10135383158922195 \n",
      "iterator 3040 : loss 0.10434992611408234 \n",
      "iterator 3050 : loss 0.09119211137294769 \n",
      "iterator 3060 : loss 0.10229774564504623 \n",
      "iterator 3070 : loss 0.1065763533115387 \n",
      "iterator 3080 : loss 0.09733612835407257 \n",
      "iterator 3090 : loss 0.10141900926828384 \n",
      "iterator 3100 : loss 0.10081462562084198 \n",
      "iterator 3110 : loss 0.10156761854887009 \n",
      "iterator 3120 : loss 0.0986306294798851 \n",
      "iterator 3130 : loss 0.08975807577371597 \n",
      "iterator 3140 : loss 0.10528938472270966 \n",
      "iterator 3150 : loss 0.09715656191110611 \n",
      "iterator 3160 : loss 0.09591970592737198 \n",
      "iterator 3170 : loss 0.09362941235303879 \n",
      "iterator 3180 : loss 0.09921437501907349 \n",
      "iterator 3190 : loss 0.10027560591697693 \n",
      "iterator 3200 : loss 0.1002405434846878 \n",
      "iterator 3210 : loss 0.0924854502081871 \n",
      "iterator 3220 : loss 0.08589421212673187 \n",
      "iterator 3230 : loss 0.09606460481882095 \n",
      "iterator 3240 : loss 0.09203218668699265 \n",
      "iterator 3250 : loss 0.10035919398069382 \n",
      "iterator 3260 : loss 0.09543759375810623 \n",
      "iterator 3270 : loss 0.08386185765266418 \n",
      "iterator 3280 : loss 0.09178058803081512 \n",
      "iterator 3290 : loss 0.09729718416929245 \n",
      "iterator 3300 : loss 0.10009520500898361 \n",
      "iterator 3310 : loss 0.09012521803379059 \n",
      "iterator 3320 : loss 0.09721560031175613 \n",
      "iterator 3330 : loss 0.0924621969461441 \n",
      "iterator 3340 : loss 0.09709570556879044 \n",
      "iterator 3350 : loss 0.08664111793041229 \n",
      "iterator 3360 : loss 0.09958573430776596 \n",
      "iterator 3370 : loss 0.09679307788610458 \n",
      "iterator 3380 : loss 0.10499734431505203 \n",
      "iterator 3390 : loss 0.10138317197561264 \n",
      "iterator 3400 : loss 0.10024674236774445 \n",
      "iterator 3410 : loss 0.09365274757146835 \n",
      "iterator 3420 : loss 0.091713547706604 \n",
      "iterator 3430 : loss 0.09892294555902481 \n",
      "iterator 3440 : loss 0.08949588239192963 \n",
      "iterator 3450 : loss 0.09915628284215927 \n",
      "iterator 3460 : loss 0.10100909322500229 \n",
      "iterator 3470 : loss 0.10048796236515045 \n",
      "iterator 3480 : loss 0.09650528430938721 \n",
      "iterator 3490 : loss 0.09741838276386261 \n",
      "iterator 3500 : loss 0.10090148448944092 \n",
      "iterator 3510 : loss 0.09903863817453384 \n",
      "iterator 3520 : loss 0.09494402259588242 \n",
      "iterator 3530 : loss 0.1008659154176712 \n",
      "iterator 3540 : loss 0.08363980799913406 \n",
      "iterator 3550 : loss 0.0939560979604721 \n",
      "iterator 3560 : loss 0.09632068127393723 \n",
      "iterator 3570 : loss 0.10489549487829208 \n",
      "iterator 3580 : loss 0.09924746304750443 \n",
      "iterator 3590 : loss 0.09906008094549179 \n",
      "iterator 3600 : loss 0.09498253464698792 \n",
      "iterator 3610 : loss 0.093172587454319 \n",
      "iterator 3620 : loss 0.09891914576292038 \n",
      "iterator 3630 : loss 0.09819937497377396 \n",
      "iterator 3640 : loss 0.10661949217319489 \n",
      "iterator 3650 : loss 0.10795184224843979 \n",
      "iterator 3660 : loss 0.09578543156385422 \n",
      "iterator 3670 : loss 0.09242531657218933 \n",
      "iterator 3680 : loss 0.08925488591194153 \n",
      "iterator 3690 : loss 0.09034185111522675 \n",
      "iterator 3700 : loss 0.09402358531951904 \n",
      "iterator 3710 : loss 0.089573934674263 \n",
      "iterator 3720 : loss 0.09202571958303452 \n",
      "iterator 3730 : loss 0.09149850904941559 \n",
      "iterator 3740 : loss 0.08963438868522644 \n",
      "iterator 3750 : loss 0.08684880286455154 \n",
      "iterator 3760 : loss 0.09894182533025742 \n",
      "iterator 3770 : loss 0.09518276900053024 \n",
      "iterator 3780 : loss 0.08920842409133911 \n",
      "iterator 3790 : loss 0.09235010296106339 \n",
      "iterator 3800 : loss 0.09433992207050323 \n",
      "iterator 3810 : loss 0.08777930587530136 \n",
      "iterator 3820 : loss 0.08816297352313995 \n",
      "iterator 3830 : loss 0.09480482339859009 \n",
      "iterator 3840 : loss 0.09948491305112839 \n",
      "iterator 3850 : loss 0.0980384573340416 \n",
      "iterator 3860 : loss 0.10433679819107056 \n",
      "iterator 3870 : loss 0.0897698923945427 \n",
      "iterator 3880 : loss 0.10048167407512665 \n",
      "iterator 3890 : loss 0.09237943589687347 \n",
      "iterator 3900 : loss 0.09470055252313614 \n",
      "iterator 3910 : loss 0.09033119678497314 \n",
      "iterator 3920 : loss 0.0942123755812645 \n",
      "iterator 3930 : loss 0.09658823162317276 \n",
      "iterator 3940 : loss 0.08622181415557861 \n",
      "iterator 3950 : loss 0.09156352281570435 \n",
      "iterator 3960 : loss 0.10006865859031677 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 3970 : loss 0.09400796890258789 \n",
      "iterator 3980 : loss 0.0960390493273735 \n",
      "iterator 3990 : loss 0.09253762662410736 \n",
      "iterator 4000 : loss 0.10059811919927597 \n",
      "iterator 4010 : loss 0.09459757059812546 \n",
      "iterator 4020 : loss 0.0909419134259224 \n",
      "iterator 4030 : loss 0.09530682116746902 \n",
      "iterator 4040 : loss 0.0891919806599617 \n",
      "iterator 4050 : loss 0.09285463392734528 \n",
      "iterator 4060 : loss 0.09043829143047333 \n",
      "iterator 4070 : loss 0.09552288055419922 \n",
      "iterator 4080 : loss 0.10282059013843536 \n",
      "iterator 4090 : loss 0.08965037018060684 \n",
      "iterator 4100 : loss 0.0867980420589447 \n",
      "iterator 4110 : loss 0.09440290927886963 \n",
      "iterator 4120 : loss 0.09634608030319214 \n",
      "iterator 4130 : loss 0.09889505058526993 \n",
      "iterator 4140 : loss 0.09276711195707321 \n",
      "iterator 4150 : loss 0.09282416105270386 \n",
      "iterator 4160 : loss 0.08880255371332169 \n",
      "iterator 4170 : loss 0.08324222266674042 \n",
      "iterator 4180 : loss 0.08974885195493698 \n",
      "iterator 4190 : loss 0.09635914862155914 \n",
      "iterator 4200 : loss 0.08758377283811569 \n",
      "iterator 4210 : loss 0.0903846025466919 \n",
      "iterator 4220 : loss 0.08668075501918793 \n",
      "iterator 4230 : loss 0.08866535872220993 \n",
      "iterator 4240 : loss 0.08907216787338257 \n",
      "iterator 4250 : loss 0.08841728419065475 \n",
      "iterator 4260 : loss 0.09517107158899307 \n",
      "iterator 4270 : loss 0.09075472503900528 \n",
      "iterator 4280 : loss 0.09477580338716507 \n",
      "iterator 4290 : loss 0.0946090966463089 \n",
      "iterator 4300 : loss 0.08851493149995804 \n",
      "iterator 4310 : loss 0.08244384080171585 \n",
      "iterator 4320 : loss 0.09858518838882446 \n",
      "iterator 4330 : loss 0.09954158961772919 \n",
      "iterator 4340 : loss 0.0946694165468216 \n",
      "iterator 4350 : loss 0.08857149630784988 \n",
      "iterator 4360 : loss 0.08891323953866959 \n",
      "iterator 4370 : loss 0.09376312047243118 \n",
      "iterator 4380 : loss 0.08849531412124634 \n",
      "iterator 4390 : loss 0.08767542243003845 \n",
      "iterator 4400 : loss 0.09004701673984528 \n",
      "iterator 4410 : loss 0.08540675789117813 \n",
      "iterator 4420 : loss 0.08737799525260925 \n",
      "iterator 4430 : loss 0.09407500922679901 \n",
      "iterator 4440 : loss 0.08698031306266785 \n",
      "iterator 4450 : loss 0.09410165250301361 \n",
      "iterator 4460 : loss 0.08544258028268814 \n",
      "iterator 4470 : loss 0.0908467248082161 \n",
      "iterator 4480 : loss 0.08048273622989655 \n",
      "iterator 4490 : loss 0.08855219930410385 \n",
      "iterator 4500 : loss 0.09928783029317856 \n",
      "iterator 4510 : loss 0.09664798527956009 \n",
      "iterator 4520 : loss 0.09459849447011948 \n",
      "iterator 4530 : loss 0.0845511183142662 \n",
      "iterator 4540 : loss 0.09385638684034348 \n",
      "iterator 4550 : loss 0.09027896821498871 \n",
      "iterator 4560 : loss 0.08746007084846497 \n",
      "iterator 4570 : loss 0.08631830662488937 \n",
      "iterator 4580 : loss 0.09153502434492111 \n",
      "iterator 4590 : loss 0.09041666239500046 \n",
      "iterator 4600 : loss 0.09247807413339615 \n",
      "iterator 4610 : loss 0.08442521840333939 \n",
      "iterator 4620 : loss 0.09519544243812561 \n",
      "iterator 4630 : loss 0.08553993701934814 \n",
      "iterator 4640 : loss 0.09862910211086273 \n",
      "iterator 4650 : loss 0.08702214062213898 \n",
      "iterator 4660 : loss 0.08874426782131195 \n",
      "iterator 4670 : loss 0.08774550259113312 \n",
      "iterator 4680 : loss 0.0873892679810524 \n",
      "iterator 4690 : loss 0.08425530791282654 \n",
      "iterator 4700 : loss 0.08638918399810791 \n",
      "iterator 4710 : loss 0.08937378227710724 \n",
      "iterator 4720 : loss 0.08655449748039246 \n",
      "iterator 4730 : loss 0.08956866711378098 \n",
      "iterator 4740 : loss 0.08975770324468613 \n",
      "iterator 4750 : loss 0.09198760986328125 \n",
      "iterator 4760 : loss 0.09029538184404373 \n",
      "iterator 4770 : loss 0.08684086054563522 \n",
      "iterator 4780 : loss 0.09495645761489868 \n",
      "iterator 4790 : loss 0.08993323147296906 \n",
      "iterator 4800 : loss 0.09195973724126816 \n",
      "iterator 4810 : loss 0.09277155250310898 \n",
      "iterator 4820 : loss 0.08603309094905853 \n",
      "iterator 4830 : loss 0.08823976665735245 \n",
      "iterator 4840 : loss 0.09352337568998337 \n",
      "iterator 4850 : loss 0.08939442038536072 \n",
      "iterator 4860 : loss 0.09005921334028244 \n",
      "iterator 4870 : loss 0.09103802591562271 \n",
      "iterator 4880 : loss 0.08701425790786743 \n",
      "iterator 4890 : loss 0.08344866335391998 \n",
      "iterator 4900 : loss 0.09900211542844772 \n",
      "iterator 4910 : loss 0.09176820516586304 \n",
      "iterator 4920 : loss 0.08831612765789032 \n",
      "iterator 4930 : loss 0.08779687434434891 \n",
      "iterator 4940 : loss 0.09815237671136856 \n",
      "iterator 4950 : loss 0.08258198946714401 \n",
      "iterator 4960 : loss 0.08641455322504044 \n",
      "iterator 4970 : loss 0.09508880972862244 \n",
      "iterator 4980 : loss 0.08846202492713928 \n",
      "iterator 4990 : loss 0.08332114666700363 \n",
      "iterator 5000 : loss 0.09544022381305695 \n",
      "iterator 5010 : loss 0.08271205425262451 \n",
      "iterator 5020 : loss 0.0825372040271759 \n",
      "iterator 5030 : loss 0.08621146529912949 \n",
      "iterator 5040 : loss 0.08970317989587784 \n",
      "iterator 5050 : loss 0.08830621093511581 \n",
      "iterator 5060 : loss 0.09377753734588623 \n",
      "iterator 5070 : loss 0.08431613445281982 \n",
      "iterator 5080 : loss 0.0822824239730835 \n",
      "iterator 5090 : loss 0.0876888707280159 \n",
      "iterator 5100 : loss 0.09334949404001236 \n",
      "iterator 5110 : loss 0.08993804454803467 \n",
      "iterator 5120 : loss 0.08362025022506714 \n",
      "iterator 5130 : loss 0.08331263810396194 \n",
      "iterator 5140 : loss 0.09256980568170547 \n",
      "iterator 5150 : loss 0.09155043959617615 \n",
      "iterator 5160 : loss 0.08186028897762299 \n",
      "iterator 5170 : loss 0.08905643969774246 \n",
      "iterator 5180 : loss 0.0816577598452568 \n",
      "iterator 5190 : loss 0.08615834265947342 \n",
      "iterator 5200 : loss 0.08946983516216278 \n",
      "iterator 5210 : loss 0.08695776760578156 \n",
      "iterator 5220 : loss 0.08873388916254044 \n",
      "iterator 5230 : loss 0.08560889214277267 \n",
      "iterator 5240 : loss 0.08197148144245148 \n",
      "iterator 5250 : loss 0.09467009454965591 \n",
      "iterator 5260 : loss 0.09262462705373764 \n",
      "iterator 5270 : loss 0.09112737327814102 \n",
      "iterator 5280 : loss 0.08988411724567413 \n",
      "iterator 5290 : loss 0.08927123248577118 \n",
      "iterator 5300 : loss 0.09164036810398102 \n",
      "iterator 5310 : loss 0.08157353848218918 \n",
      "iterator 5320 : loss 0.09087000042200089 \n",
      "iterator 5330 : loss 0.08618864417076111 \n",
      "iterator 5340 : loss 0.08841431140899658 \n",
      "iterator 5350 : loss 0.08660958707332611 \n",
      "iterator 5360 : loss 0.08644384145736694 \n",
      "iterator 5370 : loss 0.08741308748722076 \n",
      "iterator 5380 : loss 0.09115152806043625 \n",
      "iterator 5390 : loss 0.08290459215641022 \n",
      "iterator 5400 : loss 0.08473542332649231 \n",
      "iterator 5410 : loss 0.09055791795253754 \n",
      "iterator 5420 : loss 0.08484536409378052 \n",
      "iterator 5430 : loss 0.08631215989589691 \n",
      "iterator 5440 : loss 0.09072770923376083 \n",
      "iterator 5450 : loss 0.09382466226816177 \n",
      "iterator 5460 : loss 0.09315457940101624 \n",
      "iterator 5470 : loss 0.09283316880464554 \n",
      "iterator 5480 : loss 0.08687900006771088 \n",
      "iterator 5490 : loss 0.0886581540107727 \n",
      "iterator 5500 : loss 0.08051673322916031 \n",
      "iterator 5510 : loss 0.0942523330450058 \n",
      "iterator 5520 : loss 0.08783899247646332 \n",
      "iterator 5530 : loss 0.0852394700050354 \n",
      "iterator 5540 : loss 0.08920895308256149 \n",
      "iterator 5550 : loss 0.09236559271812439 \n",
      "iterator 5560 : loss 0.08546613156795502 \n",
      "iterator 5570 : loss 0.08028484880924225 \n",
      "iterator 5580 : loss 0.08030436933040619 \n",
      "iterator 5590 : loss 0.0841250941157341 \n",
      "iterator 5600 : loss 0.09068973362445831 \n",
      "iterator 5610 : loss 0.08466827869415283 \n",
      "iterator 5620 : loss 0.0890841856598854 \n",
      "iterator 5630 : loss 0.08686621487140656 \n",
      "iterator 5640 : loss 0.08443686366081238 \n",
      "iterator 5650 : loss 0.08061739057302475 \n",
      "iterator 5660 : loss 0.08374574035406113 \n",
      "iterator 5670 : loss 0.08021882176399231 \n",
      "iterator 5680 : loss 0.07920852303504944 \n",
      "iterator 5690 : loss 0.08887755125761032 \n",
      "iterator 5700 : loss 0.08674691617488861 \n",
      "iterator 5710 : loss 0.08142658323049545 \n",
      "iterator 5720 : loss 0.08730936795473099 \n",
      "iterator 5730 : loss 0.08649034053087234 \n",
      "iterator 5740 : loss 0.08620641380548477 \n",
      "iterator 5750 : loss 0.07824140042066574 \n",
      "iterator 5760 : loss 0.08679802715778351 \n",
      "iterator 5770 : loss 0.07387720793485641 \n",
      "iterator 5780 : loss 0.08369375765323639 \n",
      "iterator 5790 : loss 0.07857637107372284 \n",
      "iterator 5800 : loss 0.09181983023881912 \n",
      "iterator 5810 : loss 0.09080343693494797 \n",
      "iterator 5820 : loss 0.073494091629982 \n",
      "iterator 5830 : loss 0.08176089078187943 \n",
      "iterator 5840 : loss 0.08613873273134232 \n",
      "iterator 5850 : loss 0.07705825567245483 \n",
      "iterator 5860 : loss 0.08086980879306793 \n",
      "iterator 5870 : loss 0.07875141501426697 \n",
      "iterator 5880 : loss 0.08999751508235931 \n",
      "iterator 5890 : loss 0.08806949108839035 \n",
      "iterator 5900 : loss 0.0825081467628479 \n",
      "iterator 5910 : loss 0.08316412568092346 \n",
      "iterator 5920 : loss 0.0804942399263382 \n",
      "iterator 5930 : loss 0.08662223815917969 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 5940 : loss 0.08131643384695053 \n",
      "iterator 5950 : loss 0.08225519955158234 \n",
      "iterator 5960 : loss 0.08295846730470657 \n",
      "iterator 5970 : loss 0.08024721592664719 \n",
      "iterator 5980 : loss 0.07843803614377975 \n",
      "iterator 5990 : loss 0.08640207350254059 \n",
      "iterator 6000 : loss 0.09381627291440964 \n",
      "iterator 6010 : loss 0.08962158113718033 \n",
      "iterator 6020 : loss 0.08812065422534943 \n",
      "iterator 6030 : loss 0.08657073229551315 \n",
      "iterator 6040 : loss 0.08718080073595047 \n",
      "iterator 6050 : loss 0.08955357223749161 \n",
      "iterator 6060 : loss 0.08221939951181412 \n",
      "iterator 6070 : loss 0.08446462452411652 \n",
      "iterator 6080 : loss 0.07824325561523438 \n",
      "iterator 6090 : loss 0.0818307027220726 \n",
      "iterator 6100 : loss 0.08430317789316177 \n",
      "iterator 6110 : loss 0.07670216262340546 \n",
      "iterator 6120 : loss 0.09068098664283752 \n",
      "iterator 6130 : loss 0.08207204192876816 \n",
      "iterator 6140 : loss 0.08048445731401443 \n",
      "iterator 6150 : loss 0.08245833218097687 \n",
      "iterator 6160 : loss 0.08351049572229385 \n",
      "iterator 6170 : loss 0.08375339210033417 \n",
      "iterator 6180 : loss 0.08238092064857483 \n",
      "iterator 6190 : loss 0.08789151906967163 \n",
      "iterator 6200 : loss 0.08550777286291122 \n",
      "iterator 6210 : loss 0.08569400012493134 \n",
      "iterator 6220 : loss 0.07849161326885223 \n",
      "iterator 6230 : loss 0.0806688517332077 \n",
      "iterator 6240 : loss 0.08233392238616943 \n",
      "iterator 6250 : loss 0.08231363445520401 \n",
      "iterator 6260 : loss 0.08420755714178085 \n",
      "iterator 6270 : loss 0.09022238105535507 \n",
      "iterator 6280 : loss 0.07698901742696762 \n",
      "iterator 6290 : loss 0.08484485000371933 \n",
      "iterator 6300 : loss 0.08359070122241974 \n",
      "iterator 6310 : loss 0.0840272530913353 \n",
      "iterator 6320 : loss 0.08757863938808441 \n",
      "iterator 6330 : loss 0.07426472008228302 \n",
      "iterator 6340 : loss 0.08381476998329163 \n",
      "iterator 6350 : loss 0.08453425765037537 \n",
      "iterator 6360 : loss 0.08678192645311356 \n",
      "iterator 6370 : loss 0.08030010014772415 \n",
      "iterator 6380 : loss 0.07679762691259384 \n",
      "iterator 6390 : loss 0.08250485360622406 \n",
      "iterator 6400 : loss 0.08470672369003296 \n",
      "iterator 6410 : loss 0.07889721542596817 \n",
      "iterator 6420 : loss 0.07609335333108902 \n",
      "iterator 6430 : loss 0.08672451227903366 \n",
      "iterator 6440 : loss 0.08200715482234955 \n",
      "iterator 6450 : loss 0.0915323793888092 \n",
      "iterator 6460 : loss 0.08519643545150757 \n",
      "iterator 6470 : loss 0.08602942526340485 \n",
      "iterator 6480 : loss 0.09284594655036926 \n",
      "iterator 6490 : loss 0.08407986164093018 \n",
      "iterator 6500 : loss 0.08269132673740387 \n",
      "iterator 6510 : loss 0.08858849853277206 \n",
      "iterator 6520 : loss 0.085722915828228 \n",
      "iterator 6530 : loss 0.0797552838921547 \n",
      "iterator 6540 : loss 0.08370720595121384 \n",
      "iterator 6550 : loss 0.07928737998008728 \n",
      "iterator 6560 : loss 0.08099385350942612 \n",
      "iterator 6570 : loss 0.07304444909095764 \n",
      "iterator 6580 : loss 0.0866488441824913 \n",
      "iterator 6590 : loss 0.08271502703428268 \n",
      "iterator 6600 : loss 0.08139941841363907 \n",
      "iterator 6610 : loss 0.07426993548870087 \n",
      "iterator 6620 : loss 0.08077815920114517 \n",
      "iterator 6630 : loss 0.08728157728910446 \n",
      "iterator 6640 : loss 0.07975543290376663 \n",
      "iterator 6650 : loss 0.08274656534194946 \n",
      "iterator 6660 : loss 0.09339284151792526 \n",
      "iterator 6670 : loss 0.09281860291957855 \n",
      "iterator 6680 : loss 0.0957975760102272 \n",
      "iterator 6690 : loss 0.08081576973199844 \n",
      "iterator 6700 : loss 0.08464968949556351 \n",
      "iterator 6710 : loss 0.08169456571340561 \n",
      "iterator 6720 : loss 0.08204840868711472 \n",
      "iterator 6730 : loss 0.07741916179656982 \n",
      "iterator 6740 : loss 0.08614969998598099 \n",
      "iterator 6750 : loss 0.08123602718114853 \n",
      "iterator 6760 : loss 0.08523839712142944 \n",
      "iterator 6770 : loss 0.08078083395957947 \n",
      "iterator 6780 : loss 0.08437185734510422 \n",
      "iterator 6790 : loss 0.07930734008550644 \n",
      "iterator 6800 : loss 0.08520394563674927 \n",
      "iterator 6810 : loss 0.08219292759895325 \n",
      "iterator 6820 : loss 0.08501406013965607 \n",
      "iterator 6830 : loss 0.08014322817325592 \n",
      "iterator 6840 : loss 0.0828118845820427 \n",
      "iterator 6850 : loss 0.0866294577717781 \n",
      "iterator 6860 : loss 0.08461380004882812 \n",
      "iterator 6870 : loss 0.09135477244853973 \n",
      "iterator 6880 : loss 0.07790789753198624 \n",
      "iterator 6890 : loss 0.07994689792394638 \n",
      "iterator 6900 : loss 0.07748741656541824 \n",
      "iterator 6910 : loss 0.09100442379713058 \n",
      "iterator 6920 : loss 0.08811172097921371 \n",
      "iterator 6930 : loss 0.0886118933558464 \n",
      "iterator 6940 : loss 0.08945897966623306 \n",
      "iterator 6950 : loss 0.08119586110115051 \n",
      "iterator 6960 : loss 0.08326886594295502 \n",
      "iterator 6970 : loss 0.07733108848333359 \n",
      "iterator 6980 : loss 0.08429607003927231 \n",
      "iterator 6990 : loss 0.07494663447141647 \n",
      "iterator 7000 : loss 0.08594725281000137 \n",
      "iterator 7010 : loss 0.0771854817867279 \n",
      "iterator 7020 : loss 0.07526151090860367 \n",
      "iterator 7030 : loss 0.07897544652223587 \n",
      "iterator 7040 : loss 0.08092225342988968 \n",
      "iterator 7050 : loss 0.080144502222538 \n",
      "iterator 7060 : loss 0.07885458320379257 \n",
      "iterator 7070 : loss 0.08263463526964188 \n",
      "iterator 7080 : loss 0.07618063688278198 \n",
      "iterator 7090 : loss 0.08659157156944275 \n",
      "iterator 7100 : loss 0.09128455072641373 \n",
      "iterator 7110 : loss 0.08065938204526901 \n",
      "iterator 7120 : loss 0.07577775418758392 \n",
      "iterator 7130 : loss 0.08861973881721497 \n",
      "iterator 7140 : loss 0.08886263519525528 \n",
      "iterator 7150 : loss 0.08205274492502213 \n",
      "iterator 7160 : loss 0.08387371152639389 \n",
      "iterator 7170 : loss 0.08761783689260483 \n",
      "iterator 7180 : loss 0.07969052344560623 \n",
      "iterator 7190 : loss 0.08048959821462631 \n",
      "iterator 7200 : loss 0.08175168931484222 \n",
      "iterator 7210 : loss 0.08623631298542023 \n",
      "iterator 7220 : loss 0.07601416856050491 \n",
      "iterator 7230 : loss 0.07328537106513977 \n",
      "iterator 7240 : loss 0.07672347128391266 \n",
      "iterator 7250 : loss 0.08386027067899704 \n",
      "iterator 7260 : loss 0.07526814192533493 \n",
      "iterator 7270 : loss 0.0809168592095375 \n",
      "iterator 7280 : loss 0.08768685162067413 \n",
      "iterator 7290 : loss 0.07809768617153168 \n",
      "iterator 7300 : loss 0.0803774744272232 \n",
      "iterator 7310 : loss 0.08371340483427048 \n",
      "iterator 7320 : loss 0.09026633203029633 \n",
      "iterator 7330 : loss 0.09704989939928055 \n",
      "iterator 7340 : loss 0.08371681720018387 \n",
      "iterator 7350 : loss 0.07377555221319199 \n",
      "iterator 7360 : loss 0.07970380038022995 \n",
      "iterator 7370 : loss 0.08205696195363998 \n",
      "iterator 7380 : loss 0.08159393072128296 \n",
      "iterator 7390 : loss 0.08475847542285919 \n",
      "iterator 7400 : loss 0.08993016183376312 \n",
      "iterator 7410 : loss 0.08905830979347229 \n",
      "iterator 7420 : loss 0.08391123265028 \n",
      "iterator 7430 : loss 0.08041112124919891 \n",
      "iterator 7440 : loss 0.06963171064853668 \n",
      "iterator 7450 : loss 0.0795019268989563 \n",
      "iterator 7460 : loss 0.07844626158475876 \n",
      "iterator 7470 : loss 0.08595692366361618 \n",
      "iterator 7480 : loss 0.08092962950468063 \n",
      "iterator 7490 : loss 0.07716929912567139 \n",
      "iterator 7500 : loss 0.07630535215139389 \n",
      "iterator 7510 : loss 0.07930728048086166 \n",
      "iterator 7520 : loss 0.07478304952383041 \n",
      "iterator 7530 : loss 0.0802803784608841 \n",
      "iterator 7540 : loss 0.0855754017829895 \n",
      "iterator 7550 : loss 0.07293329387903214 \n",
      "iterator 7560 : loss 0.0867047980427742 \n",
      "iterator 7570 : loss 0.08485877513885498 \n",
      "iterator 7580 : loss 0.07906752824783325 \n",
      "iterator 7590 : loss 0.07540442794561386 \n",
      "iterator 7600 : loss 0.09073008596897125 \n",
      "iterator 7610 : loss 0.08555345982313156 \n",
      "iterator 7620 : loss 0.08581987023353577 \n",
      "iterator 7630 : loss 0.07779067754745483 \n",
      "iterator 7640 : loss 0.084426648914814 \n",
      "iterator 7650 : loss 0.08227250725030899 \n",
      "iterator 7660 : loss 0.07984602451324463 \n",
      "iterator 7670 : loss 0.08552160114049911 \n",
      "iterator 7680 : loss 0.08032779395580292 \n",
      "iterator 7690 : loss 0.07931496202945709 \n",
      "iterator 7700 : loss 0.07656946778297424 \n",
      "iterator 7710 : loss 0.07810179889202118 \n",
      "iterator 7720 : loss 0.08388545364141464 \n",
      "iterator 7730 : loss 0.07777667790651321 \n",
      "iterator 7740 : loss 0.07916222512722015 \n",
      "iterator 7750 : loss 0.0829051062464714 \n",
      "iterator 7760 : loss 0.07718066871166229 \n",
      "iterator 7770 : loss 0.07769540697336197 \n",
      "iterator 7780 : loss 0.08360354602336884 \n",
      "iterator 7790 : loss 0.09122893214225769 \n",
      "iterator 7800 : loss 0.08028821647167206 \n",
      "iterator 7810 : loss 0.07864150404930115 \n",
      "iterator 7820 : loss 0.08303222060203552 \n",
      "iterator 7830 : loss 0.08884821087121964 \n",
      "iterator 7840 : loss 0.07720544189214706 \n",
      "iterator 7850 : loss 0.08974342048168182 \n",
      "iterator 7860 : loss 0.08326476067304611 \n",
      "iterator 7870 : loss 0.09016918390989304 \n",
      "iterator 7880 : loss 0.08476610481739044 \n",
      "iterator 7890 : loss 0.07950928062200546 \n",
      "iterator 7900 : loss 0.07611647248268127 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 7910 : loss 0.07808949053287506 \n",
      "iterator 7920 : loss 0.07094507664442062 \n",
      "iterator 7930 : loss 0.07812303304672241 \n",
      "iterator 7940 : loss 0.08120867609977722 \n",
      "iterator 7950 : loss 0.07044022530317307 \n",
      "iterator 7960 : loss 0.08043450862169266 \n",
      "iterator 7970 : loss 0.07981852442026138 \n",
      "iterator 7980 : loss 0.08189874142408371 \n",
      "iterator 7990 : loss 0.08093510568141937 \n",
      "iterator 8000 : loss 0.07505544275045395 \n",
      "iterator 8010 : loss 0.08113029599189758 \n",
      "iterator 8020 : loss 0.07845116406679153 \n",
      "iterator 8030 : loss 0.08267336338758469 \n",
      "iterator 8040 : loss 0.08321713656187057 \n",
      "iterator 8050 : loss 0.08169294148683548 \n",
      "iterator 8060 : loss 0.07973244786262512 \n",
      "iterator 8070 : loss 0.08285998553037643 \n",
      "iterator 8080 : loss 0.08732790499925613 \n",
      "iterator 8090 : loss 0.08081863820552826 \n",
      "iterator 8100 : loss 0.08466799557209015 \n",
      "iterator 8110 : loss 0.08258912712335587 \n",
      "iterator 8120 : loss 0.08070957660675049 \n",
      "iterator 8130 : loss 0.07780708372592926 \n",
      "iterator 8140 : loss 0.08289103955030441 \n",
      "iterator 8150 : loss 0.08269952237606049 \n",
      "iterator 8160 : loss 0.08075734227895737 \n",
      "iterator 8170 : loss 0.07510343194007874 \n",
      "iterator 8180 : loss 0.0825841948390007 \n",
      "iterator 8190 : loss 0.08434291929006577 \n",
      "iterator 8200 : loss 0.08317471295595169 \n",
      "iterator 8210 : loss 0.07883915305137634 \n",
      "iterator 8220 : loss 0.07806751877069473 \n",
      "iterator 8230 : loss 0.07457220554351807 \n",
      "iterator 8240 : loss 0.08549699187278748 \n",
      "iterator 8250 : loss 0.08081986010074615 \n",
      "iterator 8260 : loss 0.08874308317899704 \n",
      "iterator 8270 : loss 0.08294591307640076 \n",
      "iterator 8280 : loss 0.07721421867609024 \n",
      "iterator 8290 : loss 0.08130539953708649 \n",
      "iterator 8300 : loss 0.07920585572719574 \n",
      "iterator 8310 : loss 0.08331555873155594 \n",
      "iterator 8320 : loss 0.07977695018053055 \n",
      "iterator 8330 : loss 0.07733727246522903 \n",
      "iterator 8340 : loss 0.08371426910161972 \n",
      "iterator 8350 : loss 0.08193603157997131 \n",
      "iterator 8360 : loss 0.08118292689323425 \n",
      "iterator 8370 : loss 0.07392275333404541 \n",
      "iterator 8380 : loss 0.08484429121017456 \n",
      "iterator 8390 : loss 0.07863972336053848 \n",
      "iterator 8400 : loss 0.08340516686439514 \n",
      "iterator 8410 : loss 0.0773487314581871 \n",
      "iterator 8420 : loss 0.0756426528096199 \n",
      "iterator 8430 : loss 0.07866578549146652 \n",
      "iterator 8440 : loss 0.07575331628322601 \n",
      "iterator 8450 : loss 0.07784715294837952 \n",
      "iterator 8460 : loss 0.07999534159898758 \n",
      "iterator 8470 : loss 0.07730553299188614 \n",
      "iterator 8480 : loss 0.08660106360912323 \n",
      "iterator 8490 : loss 0.07137522846460342 \n",
      "iterator 8500 : loss 0.07955847680568695 \n",
      "iterator 8510 : loss 0.07564374059438705 \n",
      "iterator 8520 : loss 0.07034259289503098 \n",
      "iterator 8530 : loss 0.07868244498968124 \n",
      "iterator 8540 : loss 0.08803189545869827 \n",
      "iterator 8550 : loss 0.08269285410642624 \n",
      "iterator 8560 : loss 0.08918478339910507 \n",
      "iterator 8570 : loss 0.0841309130191803 \n",
      "iterator 8580 : loss 0.08223654329776764 \n",
      "iterator 8590 : loss 0.09047704935073853 \n",
      "iterator 8600 : loss 0.0832352265715599 \n",
      "iterator 8610 : loss 0.08435741811990738 \n",
      "iterator 8620 : loss 0.07997400313615799 \n",
      "iterator 8630 : loss 0.07693997025489807 \n",
      "iterator 8640 : loss 0.07941260933876038 \n",
      "iterator 8650 : loss 0.0747181698679924 \n",
      "iterator 8660 : loss 0.0804620087146759 \n",
      "iterator 8670 : loss 0.0739971473813057 \n",
      "iterator 8680 : loss 0.07654785364866257 \n",
      "iterator 8690 : loss 0.08118485659360886 \n",
      "iterator 8700 : loss 0.08394410461187363 \n",
      "iterator 8710 : loss 0.07849287241697311 \n",
      "iterator 8720 : loss 0.08088334649801254 \n",
      "iterator 8730 : loss 0.08523080497980118 \n",
      "iterator 8740 : loss 0.07863052189350128 \n",
      "iterator 8750 : loss 0.08461494743824005 \n",
      "iterator 8760 : loss 0.06924740225076675 \n",
      "iterator 8770 : loss 0.07634982466697693 \n",
      "iterator 8780 : loss 0.07740471512079239 \n",
      "iterator 8790 : loss 0.0824432298541069 \n",
      "iterator 8800 : loss 0.07660458236932755 \n",
      "iterator 8810 : loss 0.08586180955171585 \n",
      "iterator 8820 : loss 0.07752079516649246 \n",
      "iterator 8830 : loss 0.0821993499994278 \n",
      "iterator 8840 : loss 0.07982919365167618 \n",
      "iterator 8850 : loss 0.0822572410106659 \n",
      "iterator 8860 : loss 0.084303118288517 \n",
      "iterator 8870 : loss 0.07883632928133011 \n",
      "iterator 8880 : loss 0.07439792156219482 \n",
      "iterator 8890 : loss 0.07938925921916962 \n",
      "iterator 8900 : loss 0.07841349393129349 \n",
      "iterator 8910 : loss 0.07643266767263412 \n",
      "iterator 8920 : loss 0.07723692804574966 \n",
      "iterator 8930 : loss 0.07632580399513245 \n",
      "iterator 8940 : loss 0.08469846099615097 \n",
      "iterator 8950 : loss 0.0798053964972496 \n",
      "iterator 8960 : loss 0.07359523326158524 \n",
      "iterator 8970 : loss 0.0799102708697319 \n",
      "iterator 8980 : loss 0.07607073336839676 \n",
      "iterator 8990 : loss 0.07844007760286331 \n",
      "iterator 9000 : loss 0.08995503187179565 \n",
      "iterator 9010 : loss 0.08050233125686646 \n",
      "iterator 9020 : loss 0.08481181412935257 \n",
      "iterator 9030 : loss 0.07982321083545685 \n",
      "iterator 9040 : loss 0.08778984844684601 \n",
      "iterator 9050 : loss 0.08194843679666519 \n",
      "iterator 9060 : loss 0.08232855051755905 \n",
      "iterator 9070 : loss 0.08384685963392258 \n",
      "iterator 9080 : loss 0.0835510864853859 \n",
      "iterator 9090 : loss 0.07926558703184128 \n",
      "iterator 9100 : loss 0.07932472974061966 \n",
      "iterator 9110 : loss 0.07322937250137329 \n",
      "iterator 9120 : loss 0.08221075683832169 \n",
      "iterator 9130 : loss 0.08553988486528397 \n",
      "iterator 9140 : loss 0.07514558732509613 \n",
      "iterator 9150 : loss 0.07997547090053558 \n",
      "iterator 9160 : loss 0.08369993418455124 \n",
      "iterator 9170 : loss 0.07978557050228119 \n",
      "iterator 9180 : loss 0.07973162829875946 \n",
      "iterator 9190 : loss 0.08526986837387085 \n",
      "iterator 9200 : loss 0.08412723243236542 \n",
      "iterator 9210 : loss 0.0853612869977951 \n",
      "iterator 9220 : loss 0.08731155842542648 \n",
      "iterator 9230 : loss 0.07687906175851822 \n",
      "iterator 9240 : loss 0.07647550106048584 \n",
      "iterator 9250 : loss 0.0752035528421402 \n",
      "iterator 9260 : loss 0.0810709223151207 \n",
      "iterator 9270 : loss 0.08041922748088837 \n",
      "iterator 9280 : loss 0.08439498394727707 \n",
      "iterator 9290 : loss 0.08765006810426712 \n",
      "iterator 9300 : loss 0.0783812552690506 \n",
      "iterator 9310 : loss 0.08410286158323288 \n",
      "iterator 9320 : loss 0.08248424530029297 \n",
      "iterator 9330 : loss 0.07859022915363312 \n",
      "iterator 9340 : loss 0.07863671332597733 \n",
      "iterator 9350 : loss 0.07306474447250366 \n",
      "iterator 9360 : loss 0.07998591661453247 \n",
      "iterator 9370 : loss 0.07801816612482071 \n",
      "iterator 9380 : loss 0.07351081073284149 \n",
      "iterator 9390 : loss 0.076584093272686 \n",
      "iterator 9400 : loss 0.07646355032920837 \n",
      "iterator 9410 : loss 0.08002126216888428 \n",
      "iterator 9420 : loss 0.08012236654758453 \n",
      "iterator 9430 : loss 0.0783955529332161 \n",
      "iterator 9440 : loss 0.07865879684686661 \n",
      "iterator 9450 : loss 0.07653241604566574 \n",
      "iterator 9460 : loss 0.07819127291440964 \n",
      "iterator 9470 : loss 0.08480099588632584 \n",
      "iterator 9480 : loss 0.07269617915153503 \n",
      "iterator 9490 : loss 0.08309195190668106 \n",
      "iterator 9500 : loss 0.07903938740491867 \n",
      "iterator 9510 : loss 0.08722196519374847 \n",
      "iterator 9520 : loss 0.07295207679271698 \n",
      "iterator 9530 : loss 0.08602621406316757 \n",
      "iterator 9540 : loss 0.08351487666368484 \n",
      "iterator 9550 : loss 0.08707408607006073 \n",
      "iterator 9560 : loss 0.07444702833890915 \n",
      "iterator 9570 : loss 0.07619652152061462 \n",
      "iterator 9580 : loss 0.08383208513259888 \n",
      "iterator 9590 : loss 0.07184971868991852 \n",
      "iterator 9600 : loss 0.08113954216241837 \n",
      "iterator 9610 : loss 0.0817531868815422 \n",
      "iterator 9620 : loss 0.07868962734937668 \n",
      "iterator 9630 : loss 0.07869935780763626 \n",
      "iterator 9640 : loss 0.07725952565670013 \n",
      "iterator 9650 : loss 0.07611832767724991 \n",
      "iterator 9660 : loss 0.07845672965049744 \n",
      "iterator 9670 : loss 0.07864272594451904 \n",
      "iterator 9680 : loss 0.07581793516874313 \n",
      "iterator 9690 : loss 0.077596515417099 \n",
      "iterator 9700 : loss 0.07922789454460144 \n",
      "iterator 9710 : loss 0.07571836560964584 \n",
      "iterator 9720 : loss 0.0721917450428009 \n",
      "iterator 9730 : loss 0.0814882442355156 \n",
      "iterator 9740 : loss 0.08487703651189804 \n",
      "iterator 9750 : loss 0.08214205503463745 \n",
      "iterator 9760 : loss 0.08084317296743393 \n",
      "iterator 9770 : loss 0.07059580832719803 \n",
      "iterator 9780 : loss 0.07620887458324432 \n",
      "iterator 9790 : loss 0.07465311884880066 \n",
      "iterator 9800 : loss 0.0779503881931305 \n",
      "iterator 9810 : loss 0.08032794296741486 \n",
      "iterator 9820 : loss 0.07833865284919739 \n",
      "iterator 9830 : loss 0.07959219068288803 \n",
      "iterator 9840 : loss 0.07813666760921478 \n",
      "iterator 9850 : loss 0.08222243934869766 \n",
      "iterator 9860 : loss 0.07314246892929077 \n",
      "iterator 9870 : loss 0.08018004894256592 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 9880 : loss 0.08197402209043503 \n",
      "iterator 9890 : loss 0.07309142500162125 \n",
      "iterator 9900 : loss 0.07865685224533081 \n",
      "iterator 9910 : loss 0.08561041206121445 \n",
      "iterator 9920 : loss 0.07856013625860214 \n",
      "iterator 9930 : loss 0.07121706753969193 \n",
      "iterator 9940 : loss 0.08278369158506393 \n",
      "iterator 9950 : loss 0.07584425806999207 \n",
      "iterator 9960 : loss 0.07856997847557068 \n",
      "iterator 9970 : loss 0.07673565298318863 \n",
      "iterator 9980 : loss 0.08443818986415863 \n",
      "iterator 9990 : loss 0.08054713904857635 \n",
      "iterator 10000 : loss 0.08277828991413116 \n"
     ]
    }
   ],
   "source": [
    "x.train(iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
