{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "layers = tf.contrib.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = os.environ['DATA_GENERATOR']\n",
    "sys.path.append(data_generator)\n",
    "import get_data\n",
    "from visualization import Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "#         regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "#         net = layers.conv2d(input, 128, (3,3), 1, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.flatten(net)\n",
    "#         net = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "#         #net = layers.fully_connected(net, 7*7, weights_regularizer=regularizer)\n",
    "#         net = tf.reshape(shape=[-1, 7, 7, 1], tensor=net)\n",
    "#         net = layers.conv2d_transpose(net, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 128, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "#         return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "#         regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "#         net = layers.conv2d(input, 128, (3,3), 1, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.flatten(net)\n",
    "        \n",
    "#         z_mu = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "#         z_logvar = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "#         epsilon = tf.random_normal(tf.shape(z_logvar), name='epsilon')\n",
    "#         z_std = tf.sqrt(tf.exp(z_logvar))\n",
    "#         latent_var = z_mu + epsilon*z_std\n",
    "        \n",
    "#         net = layers.fully_connected(latent_var, 7*7, weights_regularizer=regularizer)\n",
    "#         net = tf.reshape(shape=[-1, 7, 7, 1], tensor=net)\n",
    "#         net = layers.conv2d_transpose(net, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 128, (3,3), 2, weights_regularizer=regularizer)\n",
    "#         net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "#         return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.batch_size = 128\n",
    "        with self.graph.as_default():\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "            self.visualizer = Visualizer(exp_name='VAE_mnist', row=8, col=8)\n",
    "            self.generated_images, z_mu, z_std = self._build_graph(self.images, 8, reuse=tf.AUTO_REUSE, training=True)\n",
    "            self.recon_loss = self._recon_loss_function(self.images, self.generated_images)\n",
    "            self.kld_loss = self._kld_loss_function(z_mu, z_std)\n",
    "            self.loss = tf.reduce_mean(self.recon_loss + self.kld_loss)\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=0.0001) \\\n",
    "                           .minimize(self.loss)\n",
    "            initializer = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(initializer)\n",
    "    def train(self, iteration):\n",
    "        for i in range(iteration+1):\n",
    "            loss, recon_loss, kld_loss, _, np_real_images, np_generated_images = self.sess.run(\n",
    "                    [self.loss, self.recon_loss, self.kld_loss, self.solver, self.images, self.generated_images])\n",
    "            if i % 10 == 0:\n",
    "                print(\"iterator {} : loss {:.3} = kld {:.3} + recon {:.3} \".format(i, loss, kld_loss, recon_loss))\n",
    "            if i % 1000 == 0:\n",
    "                self.visualize(np_real_images, np_generated_images, i)\n",
    "            \n",
    "    def visualize(self, images, generated_images, i):\n",
    "        visual_imgs = np.concatenate( (images[:8*4], generated_images[:8*4]), axis = 0 )\n",
    "        visual_imgs = 255*(visual_imgs/2 + 0.5)\n",
    "        visual_imgs = visual_imgs.astype(int)\n",
    "        self.visualizer.draw_imgs(visual_imgs)\n",
    "        self.visualizer.save_fig(name=\"VAE_mnist_iter_{}\".format(str(i)))\n",
    "        \n",
    "    def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "        net = layers.conv2d(input, 128, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, 128, weights_regularizer=regularizer)\n",
    "        z_mu = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "        z_logvar = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "        epsilon = tf.random_normal(tf.shape(z_logvar), name='epsilon')\n",
    "        z_std = tf.sqrt(tf.exp(z_logvar))\n",
    "        latent_var = z_mu + epsilon*z_std\n",
    "        net = layers.fully_connected(latent_var, 128, weights_regularizer=regularizer)\n",
    "        net = layers.fully_connected(net, 7*7*32, weights_regularizer=regularizer)\n",
    "        net = tf.reshape(shape=[-1, 7, 7, 32], tensor=net)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 128, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "        return net, z_mu, z_std\n",
    "    def _recon_loss_function(self, _real_images, _generated_images):\n",
    "        recon_loss = tf.reduce_sum(tf.square(_real_images - _generated_images))\n",
    "        return recon_loss\n",
    "    def _kld_loss_function(self, z_mu, z_std):\n",
    "        #KLD_loss = -0.5 * tf.reduce_mean(1 + z_std - tf.pow(z_mu, 2) - tf.exp(z_std))\n",
    "        KLD_loss = -0.5 * tf.reduce_sum(1 -tf.square(z_mu) -tf.square(z_std) + tf.log(1e-8 + tf.square(z_std)))\n",
    "        return KLD_loss\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 0 : loss 8.6e+04 = kld 0.639 + recon 8.6e+04 \n",
      "iterator 10 : loss 8.31e+04 = kld 54.1 + recon 8.3e+04 \n",
      "iterator 20 : loss 7.38e+04 = kld 1.63e+03 + recon 7.21e+04 \n",
      "iterator 30 : loss 5.71e+04 = kld 3.48e+03 + recon 5.36e+04 \n",
      "iterator 40 : loss 4.89e+04 = kld 2.45e+03 + recon 4.65e+04 \n",
      "iterator 50 : loss 4.15e+04 = kld 6.8e+02 + recon 4.08e+04 \n",
      "iterator 60 : loss 3.56e+04 = kld 7e+02 + recon 3.49e+04 \n",
      "iterator 70 : loss 3.57e+04 = kld 4.13e+02 + recon 3.52e+04 \n",
      "iterator 80 : loss 3.49e+04 = kld 4.97e+02 + recon 3.44e+04 \n",
      "iterator 90 : loss 3.2e+04 = kld 5.67e+02 + recon 3.15e+04 \n",
      "iterator 100 : loss 3.1e+04 = kld 6.71e+02 + recon 3.04e+04 \n",
      "iterator 110 : loss 2.86e+04 = kld 8.19e+02 + recon 2.78e+04 \n",
      "iterator 120 : loss 2.61e+04 = kld 8.49e+02 + recon 2.53e+04 \n",
      "iterator 130 : loss 2.76e+04 = kld 6.97e+02 + recon 2.69e+04 \n",
      "iterator 140 : loss 2.58e+04 = kld 6.79e+02 + recon 2.51e+04 \n",
      "iterator 150 : loss 2.83e+04 = kld 5.01e+02 + recon 2.78e+04 \n",
      "iterator 160 : loss 2.57e+04 = kld 6.54e+02 + recon 2.51e+04 \n",
      "iterator 170 : loss 2.69e+04 = kld 5.77e+02 + recon 2.63e+04 \n",
      "iterator 180 : loss 2.74e+04 = kld 5.27e+02 + recon 2.69e+04 \n",
      "iterator 190 : loss 2.41e+04 = kld 5.13e+02 + recon 2.36e+04 \n",
      "iterator 200 : loss 2.61e+04 = kld 4.96e+02 + recon 2.56e+04 \n",
      "iterator 210 : loss 2.51e+04 = kld 4.29e+02 + recon 2.47e+04 \n",
      "iterator 220 : loss 2.56e+04 = kld 4.69e+02 + recon 2.52e+04 \n",
      "iterator 230 : loss 2.41e+04 = kld 5.58e+02 + recon 2.35e+04 \n",
      "iterator 240 : loss 2.35e+04 = kld 4.79e+02 + recon 2.3e+04 \n",
      "iterator 250 : loss 2.61e+04 = kld 3.21e+02 + recon 2.58e+04 \n",
      "iterator 260 : loss 2.44e+04 = kld 3.77e+02 + recon 2.4e+04 \n",
      "iterator 270 : loss 2.49e+04 = kld 3.84e+02 + recon 2.45e+04 \n",
      "iterator 280 : loss 2.53e+04 = kld 4.36e+02 + recon 2.49e+04 \n",
      "iterator 290 : loss 2.52e+04 = kld 4.8e+02 + recon 2.47e+04 \n",
      "iterator 300 : loss 2.57e+04 = kld 4.45e+02 + recon 2.53e+04 \n",
      "iterator 310 : loss 2.47e+04 = kld 4.31e+02 + recon 2.43e+04 \n",
      "iterator 320 : loss 2.34e+04 = kld 5.91e+02 + recon 2.28e+04 \n",
      "iterator 330 : loss 2.59e+04 = kld 5.36e+02 + recon 2.54e+04 \n",
      "iterator 340 : loss 2.38e+04 = kld 7.36e+02 + recon 2.31e+04 \n",
      "iterator 350 : loss 2.44e+04 = kld 6.68e+02 + recon 2.37e+04 \n",
      "iterator 360 : loss 2.31e+04 = kld 6.9e+02 + recon 2.24e+04 \n",
      "iterator 370 : loss 2.58e+04 = kld 6.32e+02 + recon 2.52e+04 \n",
      "iterator 380 : loss 2.41e+04 = kld 9.12e+02 + recon 2.31e+04 \n",
      "iterator 390 : loss 2.28e+04 = kld 8.43e+02 + recon 2.19e+04 \n",
      "iterator 400 : loss 2.49e+04 = kld 6.02e+02 + recon 2.43e+04 \n",
      "iterator 410 : loss 2.39e+04 = kld 7.75e+02 + recon 2.31e+04 \n",
      "iterator 420 : loss 2.3e+04 = kld 8.05e+02 + recon 2.22e+04 \n",
      "iterator 430 : loss 2.32e+04 = kld 9.46e+02 + recon 2.23e+04 \n",
      "iterator 440 : loss 2.32e+04 = kld 1.11e+03 + recon 2.21e+04 \n",
      "iterator 450 : loss 2.29e+04 = kld 9.67e+02 + recon 2.19e+04 \n",
      "iterator 460 : loss 2.7e+04 = kld 8.98e+02 + recon 2.61e+04 \n",
      "iterator 470 : loss 2.23e+04 = kld 1.32e+03 + recon 2.1e+04 \n",
      "iterator 480 : loss 2.26e+04 = kld 1.12e+03 + recon 2.15e+04 \n",
      "iterator 490 : loss 2.21e+04 = kld 1.15e+03 + recon 2.1e+04 \n",
      "iterator 500 : loss 2.37e+04 = kld 1.09e+03 + recon 2.27e+04 \n",
      "iterator 510 : loss 2.28e+04 = kld 1.4e+03 + recon 2.14e+04 \n",
      "iterator 520 : loss 2.19e+04 = kld 1.3e+03 + recon 2.06e+04 \n",
      "iterator 530 : loss 2.28e+04 = kld 1.02e+03 + recon 2.18e+04 \n",
      "iterator 540 : loss 2.38e+04 = kld 1.49e+03 + recon 2.23e+04 \n",
      "iterator 550 : loss 2.2e+04 = kld 1.28e+03 + recon 2.07e+04 \n",
      "iterator 560 : loss 2.19e+04 = kld 1.5e+03 + recon 2.04e+04 \n",
      "iterator 570 : loss 2.32e+04 = kld 1.42e+03 + recon 2.17e+04 \n",
      "iterator 580 : loss 2.43e+04 = kld 1.36e+03 + recon 2.29e+04 \n",
      "iterator 590 : loss 2.14e+04 = kld 1.35e+03 + recon 2.01e+04 \n",
      "iterator 600 : loss 2.25e+04 = kld 1.36e+03 + recon 2.11e+04 \n",
      "iterator 610 : loss 2.08e+04 = kld 1.28e+03 + recon 1.96e+04 \n",
      "iterator 620 : loss 2.29e+04 = kld 1.24e+03 + recon 2.17e+04 \n",
      "iterator 630 : loss 2.11e+04 = kld 1.72e+03 + recon 1.94e+04 \n",
      "iterator 640 : loss 2.17e+04 = kld 1.56e+03 + recon 2.01e+04 \n",
      "iterator 650 : loss 2.27e+04 = kld 1.49e+03 + recon 2.12e+04 \n",
      "iterator 660 : loss 2.03e+04 = kld 1.52e+03 + recon 1.88e+04 \n",
      "iterator 670 : loss 1.97e+04 = kld 1.7e+03 + recon 1.8e+04 \n",
      "iterator 680 : loss 2.06e+04 = kld 1.51e+03 + recon 1.91e+04 \n",
      "iterator 690 : loss 2.11e+04 = kld 1.76e+03 + recon 1.93e+04 \n",
      "iterator 700 : loss 1.92e+04 = kld 1.91e+03 + recon 1.73e+04 \n",
      "iterator 710 : loss 1.94e+04 = kld 1.7e+03 + recon 1.77e+04 \n",
      "iterator 720 : loss 2.14e+04 = kld 1.81e+03 + recon 1.95e+04 \n",
      "iterator 730 : loss 2.01e+04 = kld 1.99e+03 + recon 1.81e+04 \n",
      "iterator 740 : loss 1.94e+04 = kld 2.08e+03 + recon 1.73e+04 \n",
      "iterator 750 : loss 2.23e+04 = kld 1.76e+03 + recon 2.05e+04 \n",
      "iterator 760 : loss 2.15e+04 = kld 2.07e+03 + recon 1.94e+04 \n",
      "iterator 770 : loss 2.06e+04 = kld 1.76e+03 + recon 1.88e+04 \n",
      "iterator 780 : loss 1.97e+04 = kld 2.16e+03 + recon 1.75e+04 \n",
      "iterator 790 : loss 2.12e+04 = kld 1.59e+03 + recon 1.96e+04 \n",
      "iterator 800 : loss 2e+04 = kld 2.17e+03 + recon 1.78e+04 \n",
      "iterator 810 : loss 1.93e+04 = kld 1.94e+03 + recon 1.73e+04 \n",
      "iterator 820 : loss 2.01e+04 = kld 1.86e+03 + recon 1.82e+04 \n",
      "iterator 830 : loss 2.03e+04 = kld 1.95e+03 + recon 1.84e+04 \n",
      "iterator 840 : loss 1.96e+04 = kld 2.14e+03 + recon 1.74e+04 \n",
      "iterator 850 : loss 2.03e+04 = kld 2.18e+03 + recon 1.81e+04 \n",
      "iterator 860 : loss 1.96e+04 = kld 1.84e+03 + recon 1.78e+04 \n",
      "iterator 870 : loss 1.94e+04 = kld 2.13e+03 + recon 1.73e+04 \n",
      "iterator 880 : loss 1.86e+04 = kld 1.93e+03 + recon 1.66e+04 \n",
      "iterator 890 : loss 1.89e+04 = kld 1.94e+03 + recon 1.7e+04 \n",
      "iterator 900 : loss 1.97e+04 = kld 2.06e+03 + recon 1.77e+04 \n",
      "iterator 910 : loss 1.97e+04 = kld 2.09e+03 + recon 1.76e+04 \n",
      "iterator 920 : loss 1.84e+04 = kld 2.16e+03 + recon 1.62e+04 \n",
      "iterator 930 : loss 1.99e+04 = kld 1.81e+03 + recon 1.81e+04 \n",
      "iterator 940 : loss 2.03e+04 = kld 2.2e+03 + recon 1.81e+04 \n",
      "iterator 950 : loss 1.85e+04 = kld 2.31e+03 + recon 1.61e+04 \n",
      "iterator 960 : loss 1.93e+04 = kld 2.1e+03 + recon 1.72e+04 \n",
      "iterator 970 : loss 2.03e+04 = kld 2e+03 + recon 1.83e+04 \n",
      "iterator 980 : loss 2e+04 = kld 2.33e+03 + recon 1.77e+04 \n",
      "iterator 990 : loss 1.86e+04 = kld 1.97e+03 + recon 1.66e+04 \n",
      "iterator 1000 : loss 1.98e+04 = kld 1.88e+03 + recon 1.79e+04 \n",
      "iterator 1010 : loss 1.99e+04 = kld 2.3e+03 + recon 1.76e+04 \n",
      "iterator 1020 : loss 1.95e+04 = kld 2.21e+03 + recon 1.73e+04 \n",
      "iterator 1030 : loss 2.02e+04 = kld 2.1e+03 + recon 1.81e+04 \n",
      "iterator 1040 : loss 1.93e+04 = kld 2.11e+03 + recon 1.71e+04 \n",
      "iterator 1050 : loss 2.09e+04 = kld 2.34e+03 + recon 1.86e+04 \n",
      "iterator 1060 : loss 2.01e+04 = kld 2.06e+03 + recon 1.8e+04 \n",
      "iterator 1070 : loss 1.94e+04 = kld 2.01e+03 + recon 1.74e+04 \n",
      "iterator 1080 : loss 1.83e+04 = kld 2.19e+03 + recon 1.61e+04 \n",
      "iterator 1090 : loss 2.06e+04 = kld 2.06e+03 + recon 1.85e+04 \n",
      "iterator 1100 : loss 1.99e+04 = kld 2.16e+03 + recon 1.78e+04 \n",
      "iterator 1110 : loss 2.03e+04 = kld 2.22e+03 + recon 1.81e+04 \n",
      "iterator 1120 : loss 2.03e+04 = kld 2.18e+03 + recon 1.81e+04 \n",
      "iterator 1130 : loss 1.85e+04 = kld 2.29e+03 + recon 1.62e+04 \n",
      "iterator 1140 : loss 1.85e+04 = kld 2.26e+03 + recon 1.62e+04 \n",
      "iterator 1150 : loss 1.89e+04 = kld 2.17e+03 + recon 1.67e+04 \n",
      "iterator 1160 : loss 2.06e+04 = kld 2.22e+03 + recon 1.84e+04 \n",
      "iterator 1170 : loss 1.9e+04 = kld 2.04e+03 + recon 1.69e+04 \n",
      "iterator 1180 : loss 1.88e+04 = kld 2.2e+03 + recon 1.66e+04 \n",
      "iterator 1190 : loss 1.92e+04 = kld 2.15e+03 + recon 1.7e+04 \n",
      "iterator 1200 : loss 1.88e+04 = kld 2.26e+03 + recon 1.66e+04 \n",
      "iterator 1210 : loss 1.88e+04 = kld 2.26e+03 + recon 1.65e+04 \n",
      "iterator 1220 : loss 2e+04 = kld 2.06e+03 + recon 1.79e+04 \n",
      "iterator 1230 : loss 2.02e+04 = kld 2.39e+03 + recon 1.78e+04 \n",
      "iterator 1240 : loss 2e+04 = kld 2.04e+03 + recon 1.79e+04 \n",
      "iterator 1250 : loss 1.95e+04 = kld 2.36e+03 + recon 1.71e+04 \n",
      "iterator 1260 : loss 1.92e+04 = kld 2.07e+03 + recon 1.71e+04 \n",
      "iterator 1270 : loss 1.92e+04 = kld 2.24e+03 + recon 1.7e+04 \n",
      "iterator 1280 : loss 1.97e+04 = kld 2.15e+03 + recon 1.75e+04 \n",
      "iterator 1290 : loss 1.83e+04 = kld 2.2e+03 + recon 1.61e+04 \n",
      "iterator 1300 : loss 1.94e+04 = kld 2.12e+03 + recon 1.73e+04 \n",
      "iterator 1310 : loss 1.97e+04 = kld 2.35e+03 + recon 1.74e+04 \n",
      "iterator 1320 : loss 1.89e+04 = kld 2.19e+03 + recon 1.67e+04 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 1330 : loss 2.01e+04 = kld 2.19e+03 + recon 1.79e+04 \n",
      "iterator 1340 : loss 1.8e+04 = kld 2.38e+03 + recon 1.57e+04 \n",
      "iterator 1350 : loss 1.86e+04 = kld 2.1e+03 + recon 1.65e+04 \n",
      "iterator 1360 : loss 1.91e+04 = kld 2.11e+03 + recon 1.7e+04 \n",
      "iterator 1370 : loss 1.9e+04 = kld 2.33e+03 + recon 1.67e+04 \n",
      "iterator 1380 : loss 1.87e+04 = kld 2.01e+03 + recon 1.67e+04 \n",
      "iterator 1390 : loss 1.88e+04 = kld 2.05e+03 + recon 1.67e+04 \n",
      "iterator 1400 : loss 2.05e+04 = kld 2.12e+03 + recon 1.84e+04 \n",
      "iterator 1410 : loss 1.93e+04 = kld 2.06e+03 + recon 1.73e+04 \n",
      "iterator 1420 : loss 1.9e+04 = kld 2.44e+03 + recon 1.65e+04 \n",
      "iterator 1430 : loss 1.91e+04 = kld 2.1e+03 + recon 1.7e+04 \n",
      "iterator 1440 : loss 2.01e+04 = kld 2.22e+03 + recon 1.78e+04 \n",
      "iterator 1450 : loss 1.93e+04 = kld 2.46e+03 + recon 1.69e+04 \n",
      "iterator 1460 : loss 1.75e+04 = kld 2.28e+03 + recon 1.53e+04 \n",
      "iterator 1470 : loss 2e+04 = kld 2.06e+03 + recon 1.79e+04 \n",
      "iterator 1480 : loss 2.05e+04 = kld 2.09e+03 + recon 1.84e+04 \n",
      "iterator 1490 : loss 1.86e+04 = kld 2.54e+03 + recon 1.6e+04 \n",
      "iterator 1500 : loss 1.99e+04 = kld 2.16e+03 + recon 1.77e+04 \n",
      "iterator 1510 : loss 2.02e+04 = kld 2.31e+03 + recon 1.79e+04 \n",
      "iterator 1520 : loss 1.94e+04 = kld 2.21e+03 + recon 1.72e+04 \n",
      "iterator 1530 : loss 1.87e+04 = kld 2.26e+03 + recon 1.64e+04 \n",
      "iterator 1540 : loss 1.94e+04 = kld 1.85e+03 + recon 1.75e+04 \n",
      "iterator 1550 : loss 1.81e+04 = kld 2.48e+03 + recon 1.56e+04 \n",
      "iterator 1560 : loss 2e+04 = kld 2.07e+03 + recon 1.79e+04 \n",
      "iterator 1570 : loss 1.93e+04 = kld 2.4e+03 + recon 1.68e+04 \n",
      "iterator 1580 : loss 1.93e+04 = kld 2.18e+03 + recon 1.72e+04 \n",
      "iterator 1590 : loss 1.92e+04 = kld 2.37e+03 + recon 1.68e+04 \n",
      "iterator 1600 : loss 1.89e+04 = kld 2.5e+03 + recon 1.64e+04 \n",
      "iterator 1610 : loss 1.89e+04 = kld 1.98e+03 + recon 1.69e+04 \n",
      "iterator 1620 : loss 1.94e+04 = kld 2.13e+03 + recon 1.72e+04 \n",
      "iterator 1630 : loss 1.95e+04 = kld 2.28e+03 + recon 1.72e+04 \n",
      "iterator 1640 : loss 1.82e+04 = kld 2.06e+03 + recon 1.61e+04 \n",
      "iterator 1650 : loss 1.82e+04 = kld 2.25e+03 + recon 1.59e+04 \n",
      "iterator 1660 : loss 1.93e+04 = kld 2.15e+03 + recon 1.71e+04 \n",
      "iterator 1670 : loss 1.84e+04 = kld 2.26e+03 + recon 1.62e+04 \n",
      "iterator 1680 : loss 1.93e+04 = kld 2.15e+03 + recon 1.72e+04 \n",
      "iterator 1690 : loss 2e+04 = kld 2.43e+03 + recon 1.76e+04 \n",
      "iterator 1700 : loss 1.84e+04 = kld 2.13e+03 + recon 1.63e+04 \n",
      "iterator 1710 : loss 1.86e+04 = kld 2.61e+03 + recon 1.6e+04 \n",
      "iterator 1720 : loss 1.96e+04 = kld 2.04e+03 + recon 1.75e+04 \n",
      "iterator 1730 : loss 1.87e+04 = kld 2.19e+03 + recon 1.65e+04 \n",
      "iterator 1740 : loss 1.8e+04 = kld 2.54e+03 + recon 1.55e+04 \n",
      "iterator 1750 : loss 1.88e+04 = kld 2.24e+03 + recon 1.66e+04 \n",
      "iterator 1760 : loss 1.83e+04 = kld 2.29e+03 + recon 1.6e+04 \n",
      "iterator 1770 : loss 1.88e+04 = kld 2.38e+03 + recon 1.64e+04 \n",
      "iterator 1780 : loss 1.87e+04 = kld 2.55e+03 + recon 1.62e+04 \n",
      "iterator 1790 : loss 2.02e+04 = kld 2.2e+03 + recon 1.8e+04 \n",
      "iterator 1800 : loss 1.84e+04 = kld 2.4e+03 + recon 1.6e+04 \n",
      "iterator 1810 : loss 1.94e+04 = kld 2.12e+03 + recon 1.72e+04 \n",
      "iterator 1820 : loss 1.79e+04 = kld 2.23e+03 + recon 1.56e+04 \n",
      "iterator 1830 : loss 1.8e+04 = kld 2.43e+03 + recon 1.55e+04 \n",
      "iterator 1840 : loss 1.84e+04 = kld 2.5e+03 + recon 1.59e+04 \n",
      "iterator 1850 : loss 1.88e+04 = kld 2.21e+03 + recon 1.66e+04 \n",
      "iterator 1860 : loss 1.82e+04 = kld 2.05e+03 + recon 1.61e+04 \n",
      "iterator 1870 : loss 1.91e+04 = kld 2.51e+03 + recon 1.65e+04 \n",
      "iterator 1880 : loss 1.83e+04 = kld 2.53e+03 + recon 1.58e+04 \n",
      "iterator 1890 : loss 1.93e+04 = kld 2.29e+03 + recon 1.7e+04 \n",
      "iterator 1900 : loss 1.87e+04 = kld 2.04e+03 + recon 1.67e+04 \n",
      "iterator 1910 : loss 1.82e+04 = kld 2.21e+03 + recon 1.6e+04 \n",
      "iterator 1920 : loss 1.92e+04 = kld 2.46e+03 + recon 1.67e+04 \n",
      "iterator 1930 : loss 1.74e+04 = kld 2.11e+03 + recon 1.53e+04 \n",
      "iterator 1940 : loss 1.98e+04 = kld 2.07e+03 + recon 1.77e+04 \n",
      "iterator 1950 : loss 1.87e+04 = kld 2.27e+03 + recon 1.64e+04 \n",
      "iterator 1960 : loss 1.84e+04 = kld 2.5e+03 + recon 1.59e+04 \n",
      "iterator 1970 : loss 1.95e+04 = kld 2.25e+03 + recon 1.73e+04 \n",
      "iterator 1980 : loss 1.89e+04 = kld 2.22e+03 + recon 1.67e+04 \n",
      "iterator 1990 : loss 1.86e+04 = kld 2.5e+03 + recon 1.61e+04 \n",
      "iterator 2000 : loss 1.85e+04 = kld 2.24e+03 + recon 1.63e+04 \n",
      "iterator 2010 : loss 1.91e+04 = kld 2.21e+03 + recon 1.68e+04 \n",
      "iterator 2020 : loss 1.82e+04 = kld 2.45e+03 + recon 1.57e+04 \n",
      "iterator 2030 : loss 1.98e+04 = kld 2.23e+03 + recon 1.76e+04 \n",
      "iterator 2040 : loss 1.86e+04 = kld 2.32e+03 + recon 1.63e+04 \n",
      "iterator 2050 : loss 1.97e+04 = kld 2.36e+03 + recon 1.73e+04 \n",
      "iterator 2060 : loss 1.9e+04 = kld 2.06e+03 + recon 1.69e+04 \n",
      "iterator 2070 : loss 1.84e+04 = kld 2.76e+03 + recon 1.56e+04 \n",
      "iterator 2080 : loss 1.88e+04 = kld 2.04e+03 + recon 1.67e+04 \n",
      "iterator 2090 : loss 1.87e+04 = kld 2.36e+03 + recon 1.64e+04 \n",
      "iterator 2100 : loss 2e+04 = kld 2.36e+03 + recon 1.76e+04 \n",
      "iterator 2110 : loss 1.86e+04 = kld 2.09e+03 + recon 1.65e+04 \n",
      "iterator 2120 : loss 1.84e+04 = kld 2.56e+03 + recon 1.58e+04 \n",
      "iterator 2130 : loss 1.84e+04 = kld 2.41e+03 + recon 1.6e+04 \n",
      "iterator 2140 : loss 1.77e+04 = kld 2.46e+03 + recon 1.52e+04 \n",
      "iterator 2150 : loss 1.8e+04 = kld 2.26e+03 + recon 1.57e+04 \n",
      "iterator 2160 : loss 1.88e+04 = kld 2.3e+03 + recon 1.65e+04 \n",
      "iterator 2170 : loss 1.92e+04 = kld 2.26e+03 + recon 1.7e+04 \n",
      "iterator 2180 : loss 1.88e+04 = kld 2.52e+03 + recon 1.63e+04 \n",
      "iterator 2190 : loss 1.86e+04 = kld 2.09e+03 + recon 1.65e+04 \n",
      "iterator 2200 : loss 1.91e+04 = kld 2.16e+03 + recon 1.69e+04 \n",
      "iterator 2210 : loss 1.82e+04 = kld 2.46e+03 + recon 1.58e+04 \n",
      "iterator 2220 : loss 1.8e+04 = kld 2.13e+03 + recon 1.58e+04 \n",
      "iterator 2230 : loss 1.89e+04 = kld 2.07e+03 + recon 1.68e+04 \n",
      "iterator 2240 : loss 1.85e+04 = kld 2.34e+03 + recon 1.61e+04 \n",
      "iterator 2250 : loss 1.97e+04 = kld 2.5e+03 + recon 1.72e+04 \n",
      "iterator 2260 : loss 2.01e+04 = kld 2.18e+03 + recon 1.8e+04 \n",
      "iterator 2270 : loss 1.87e+04 = kld 2.41e+03 + recon 1.63e+04 \n",
      "iterator 2280 : loss 1.78e+04 = kld 2.41e+03 + recon 1.54e+04 \n",
      "iterator 2290 : loss 1.78e+04 = kld 2.41e+03 + recon 1.54e+04 \n",
      "iterator 2300 : loss 1.9e+04 = kld 2.41e+03 + recon 1.66e+04 \n",
      "iterator 2310 : loss 1.89e+04 = kld 2.43e+03 + recon 1.65e+04 \n",
      "iterator 2320 : loss 1.87e+04 = kld 2.04e+03 + recon 1.66e+04 \n",
      "iterator 2330 : loss 1.71e+04 = kld 2.26e+03 + recon 1.48e+04 \n",
      "iterator 2340 : loss 1.94e+04 = kld 2.35e+03 + recon 1.71e+04 \n",
      "iterator 2350 : loss 1.97e+04 = kld 2.18e+03 + recon 1.75e+04 \n",
      "iterator 2360 : loss 1.81e+04 = kld 2.54e+03 + recon 1.56e+04 \n",
      "iterator 2370 : loss 1.93e+04 = kld 2.27e+03 + recon 1.7e+04 \n",
      "iterator 2380 : loss 1.97e+04 = kld 2.31e+03 + recon 1.74e+04 \n",
      "iterator 2390 : loss 1.9e+04 = kld 2.58e+03 + recon 1.64e+04 \n",
      "iterator 2400 : loss 1.89e+04 = kld 2.28e+03 + recon 1.66e+04 \n",
      "iterator 2410 : loss 1.91e+04 = kld 2.47e+03 + recon 1.67e+04 \n",
      "iterator 2420 : loss 1.94e+04 = kld 2.37e+03 + recon 1.7e+04 \n",
      "iterator 2430 : loss 1.82e+04 = kld 2.49e+03 + recon 1.57e+04 \n",
      "iterator 2440 : loss 1.86e+04 = kld 2.35e+03 + recon 1.62e+04 \n",
      "iterator 2450 : loss 2.05e+04 = kld 2.31e+03 + recon 1.82e+04 \n",
      "iterator 2460 : loss 1.82e+04 = kld 2.48e+03 + recon 1.57e+04 \n",
      "iterator 2470 : loss 1.78e+04 = kld 2.36e+03 + recon 1.55e+04 \n",
      "iterator 2480 : loss 1.75e+04 = kld 2.47e+03 + recon 1.5e+04 \n",
      "iterator 2490 : loss 1.89e+04 = kld 2.38e+03 + recon 1.65e+04 \n",
      "iterator 2500 : loss 1.88e+04 = kld 2.34e+03 + recon 1.64e+04 \n",
      "iterator 2510 : loss 1.81e+04 = kld 2.54e+03 + recon 1.56e+04 \n",
      "iterator 2520 : loss 2.01e+04 = kld 2.28e+03 + recon 1.78e+04 \n",
      "iterator 2530 : loss 1.85e+04 = kld 2.41e+03 + recon 1.61e+04 \n",
      "iterator 2540 : loss 1.96e+04 = kld 2.35e+03 + recon 1.72e+04 \n",
      "iterator 2550 : loss 1.84e+04 = kld 2.2e+03 + recon 1.62e+04 \n",
      "iterator 2560 : loss 1.9e+04 = kld 2.41e+03 + recon 1.66e+04 \n",
      "iterator 2570 : loss 1.83e+04 = kld 2.58e+03 + recon 1.57e+04 \n",
      "iterator 2580 : loss 1.8e+04 = kld 2.25e+03 + recon 1.57e+04 \n",
      "iterator 2590 : loss 1.86e+04 = kld 2.37e+03 + recon 1.62e+04 \n",
      "iterator 2600 : loss 1.88e+04 = kld 2.23e+03 + recon 1.66e+04 \n",
      "iterator 2610 : loss 1.89e+04 = kld 2.39e+03 + recon 1.65e+04 \n",
      "iterator 2620 : loss 1.89e+04 = kld 2.37e+03 + recon 1.65e+04 \n",
      "iterator 2630 : loss 1.9e+04 = kld 2.65e+03 + recon 1.64e+04 \n",
      "iterator 2640 : loss 1.88e+04 = kld 2.13e+03 + recon 1.67e+04 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 2650 : loss 1.77e+04 = kld 2.5e+03 + recon 1.52e+04 \n",
      "iterator 2660 : loss 1.86e+04 = kld 2.3e+03 + recon 1.63e+04 \n",
      "iterator 2670 : loss 1.85e+04 = kld 2.59e+03 + recon 1.6e+04 \n",
      "iterator 2680 : loss 1.77e+04 = kld 2.91e+03 + recon 1.48e+04 \n",
      "iterator 2690 : loss 1.89e+04 = kld 2.24e+03 + recon 1.67e+04 \n",
      "iterator 2700 : loss 1.71e+04 = kld 2.22e+03 + recon 1.49e+04 \n",
      "iterator 2710 : loss 1.82e+04 = kld 2.38e+03 + recon 1.58e+04 \n",
      "iterator 2720 : loss 1.94e+04 = kld 2.56e+03 + recon 1.68e+04 \n",
      "iterator 2730 : loss 1.9e+04 = kld 2.12e+03 + recon 1.68e+04 \n",
      "iterator 2740 : loss 1.82e+04 = kld 2.83e+03 + recon 1.54e+04 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6ae1eecb746e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-161a6277707f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iteration)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             loss, recon_loss, kld_loss, _, np_real_images, np_generated_images = self.sess.run(\n\u001b[0;32m---> 21\u001b[0;31m                     [self.loss, self.recon_loss, self.kld_loss, self.solver, self.images, self.generated_images])\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterator {} : loss {:.3} = kld {:.3} + recon {:.3} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x.train(iteration=20000)\n",
    "x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dancsalo/TensorFlow-VAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
