{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = os.environ['DATA_GENERATOR']\n",
    "sys.path.append(data_generator)\n",
    "import get_data\n",
    "from visualization import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "# sess = tf.Session()\n",
    "# x = sess.run(images)\n",
    "# print(x.shape)\n",
    "# print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.batch_size = 128\n",
    "        with self.graph.as_default():\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.images, labels = get_data.data_pipeline('mnist', 128, 'train')\n",
    "            self.visualizer = Visualizer(exp_name='AE_mnist', row=8, col=8)\n",
    "            self.generated_images = self._build_graph(self.images, 5, reuse=tf.AUTO_REUSE, training=True)\n",
    "            self.loss = self._loss_function(self.images, self.generated_images)\n",
    "            self.solver = tf.train.AdamOptimizer(learning_rate=0.0001) \\\n",
    "                           .minimize(self.loss)\n",
    "            initializer = tf.global_variables_initializer()\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(initializer)\n",
    "    def train(self, iteration):\n",
    "        for i in range(iteration+1):\n",
    "            loss, _, np_real_images, np_generated_images = self.sess.run(\n",
    "                    [self.loss, self.solver, self.images, self.generated_images])\n",
    "            if i % 10 == 0:\n",
    "                print(\"iterator {} : loss {} \".format(i, loss))\n",
    "            if i % 1000 == 0:\n",
    "                self.visualize(np_real_images, np_generated_images, i)\n",
    "            \n",
    "    def visualize(self, images, generated_images, i):\n",
    "        visual_imgs = np.concatenate( (images[:8*4], generated_images[:8*4]), axis = 0 )\n",
    "        visual_imgs = 255*(visual_imgs/2 + 0.5)\n",
    "        visual_imgs = visual_imgs.astype(int)\n",
    "        self.visualizer.draw_imgs(visual_imgs)\n",
    "        self.visualizer.save_fig(name=\"AE_mnist_iter_{}\".format(str(i)))\n",
    "        \n",
    "    def _build_graph(self, input, dim_code, reuse=tf.AUTO_REUSE, training=False):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "        net = layers.conv2d(input, 128, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.flatten(net)\n",
    "        net = layers.fully_connected(net, dim_code, weights_regularizer=regularizer)\n",
    "        #net = layers.fully_connected(net, 7*7, weights_regularizer=regularizer)\n",
    "        net = tf.reshape(shape=[-1, 7, 7, 1], tensor=net)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 1, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 64, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 128, (3,3), 2, weights_regularizer=regularizer)\n",
    "        net = layers.conv2d_transpose(net, 1, (3,3), 1, activation_fn=tf.nn.tanh, weights_regularizer=regularizer)\n",
    "        return generated\n",
    "    def _loss_function(self, _real_images, _generated_images):\n",
    "        recon_loss = tf.reduce_mean(tf.square(_real_images - _generated_images))\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n",
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28, 1), dtype=float32, device=/device:CPU:0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 0 : loss 1.238569736480713 \n",
      "iterator 10 : loss 1.0269526243209839 \n",
      "iterator 20 : loss 0.9332972764968872 \n",
      "iterator 30 : loss 0.8113276958465576 \n",
      "iterator 40 : loss 0.7787215709686279 \n",
      "iterator 50 : loss 0.7339696288108826 \n",
      "iterator 60 : loss 0.6869446039199829 \n",
      "iterator 70 : loss 0.6609533429145813 \n",
      "iterator 80 : loss 0.625163197517395 \n",
      "iterator 90 : loss 0.6171665191650391 \n",
      "iterator 100 : loss 0.6036478877067566 \n",
      "iterator 110 : loss 0.5821874141693115 \n",
      "iterator 120 : loss 0.5381113886833191 \n",
      "iterator 130 : loss 0.5315536856651306 \n",
      "iterator 140 : loss 0.5145906209945679 \n",
      "iterator 150 : loss 0.5187819600105286 \n",
      "iterator 160 : loss 0.4854039251804352 \n",
      "iterator 170 : loss 0.47969475388526917 \n",
      "iterator 180 : loss 0.45196351408958435 \n",
      "iterator 190 : loss 0.4523376226425171 \n",
      "iterator 200 : loss 0.42991411685943604 \n",
      "iterator 210 : loss 0.42651990056037903 \n",
      "iterator 220 : loss 0.4149482846260071 \n",
      "iterator 230 : loss 0.39107784628868103 \n",
      "iterator 240 : loss 0.38513174653053284 \n",
      "iterator 250 : loss 0.3940621316432953 \n",
      "iterator 260 : loss 0.39023199677467346 \n",
      "iterator 270 : loss 0.372283399105072 \n",
      "iterator 280 : loss 0.38449159264564514 \n",
      "iterator 290 : loss 0.3694990575313568 \n",
      "iterator 300 : loss 0.33828407526016235 \n",
      "iterator 310 : loss 0.3356769382953644 \n",
      "iterator 320 : loss 0.3519347012042999 \n",
      "iterator 330 : loss 0.35621923208236694 \n",
      "iterator 340 : loss 0.3298870027065277 \n",
      "iterator 350 : loss 0.3502834141254425 \n",
      "iterator 360 : loss 0.3138997554779053 \n",
      "iterator 370 : loss 0.3448004722595215 \n",
      "iterator 380 : loss 0.31172698736190796 \n",
      "iterator 390 : loss 0.3228858709335327 \n",
      "iterator 400 : loss 0.33552971482276917 \n",
      "iterator 410 : loss 0.30118831992149353 \n",
      "iterator 420 : loss 0.30166497826576233 \n",
      "iterator 430 : loss 0.2925786077976227 \n",
      "iterator 440 : loss 0.3016085624694824 \n",
      "iterator 450 : loss 0.29692238569259644 \n",
      "iterator 460 : loss 0.3090225160121918 \n",
      "iterator 470 : loss 0.28386229276657104 \n",
      "iterator 480 : loss 0.2881993353366852 \n",
      "iterator 490 : loss 0.28542959690093994 \n",
      "iterator 500 : loss 0.2847968637943268 \n",
      "iterator 510 : loss 0.2911635935306549 \n",
      "iterator 520 : loss 0.27809566259384155 \n",
      "iterator 530 : loss 0.26749563217163086 \n",
      "iterator 540 : loss 0.2740834355354309 \n",
      "iterator 550 : loss 0.2705371677875519 \n",
      "iterator 560 : loss 0.2677474319934845 \n",
      "iterator 570 : loss 0.27892932295799255 \n",
      "iterator 580 : loss 0.28201112151145935 \n",
      "iterator 590 : loss 0.2492176741361618 \n",
      "iterator 600 : loss 0.26510629057884216 \n",
      "iterator 610 : loss 0.2529010474681854 \n",
      "iterator 620 : loss 0.2507752776145935 \n",
      "iterator 630 : loss 0.24959377944469452 \n",
      "iterator 640 : loss 0.23795314133167267 \n",
      "iterator 650 : loss 0.24143290519714355 \n",
      "iterator 660 : loss 0.2196524292230606 \n",
      "iterator 670 : loss 0.224551722407341 \n",
      "iterator 680 : loss 0.23771452903747559 \n",
      "iterator 690 : loss 0.22052620351314545 \n",
      "iterator 700 : loss 0.21693933010101318 \n",
      "iterator 710 : loss 0.21505209803581238 \n",
      "iterator 720 : loss 0.2260468602180481 \n",
      "iterator 730 : loss 0.20654438436031342 \n",
      "iterator 740 : loss 0.2132072001695633 \n",
      "iterator 750 : loss 0.21925409138202667 \n",
      "iterator 760 : loss 0.20906491577625275 \n",
      "iterator 770 : loss 0.20855063199996948 \n",
      "iterator 780 : loss 0.2105182707309723 \n",
      "iterator 790 : loss 0.2118767648935318 \n",
      "iterator 800 : loss 0.20471063256263733 \n",
      "iterator 810 : loss 0.19777297973632812 \n",
      "iterator 820 : loss 0.2007008194923401 \n",
      "iterator 830 : loss 0.2055891901254654 \n",
      "iterator 840 : loss 0.20872089266777039 \n",
      "iterator 850 : loss 0.20402581989765167 \n",
      "iterator 860 : loss 0.18829746544361115 \n",
      "iterator 870 : loss 0.1888803243637085 \n",
      "iterator 880 : loss 0.18541908264160156 \n",
      "iterator 890 : loss 0.19922684133052826 \n",
      "iterator 900 : loss 0.1898103803396225 \n",
      "iterator 910 : loss 0.1934739351272583 \n",
      "iterator 920 : loss 0.192074716091156 \n",
      "iterator 930 : loss 0.18701572716236115 \n",
      "iterator 940 : loss 0.18209823966026306 \n",
      "iterator 950 : loss 0.17696678638458252 \n",
      "iterator 960 : loss 0.1751493662595749 \n",
      "iterator 970 : loss 0.17964807152748108 \n",
      "iterator 980 : loss 0.19451238214969635 \n",
      "iterator 990 : loss 0.17054298520088196 \n",
      "iterator 1000 : loss 0.1850968599319458 \n",
      "iterator 1010 : loss 0.19420427083969116 \n",
      "iterator 1020 : loss 0.18731828033924103 \n",
      "iterator 1030 : loss 0.19062817096710205 \n",
      "iterator 1040 : loss 0.1949981451034546 \n",
      "iterator 1050 : loss 0.1761208027601242 \n",
      "iterator 1060 : loss 0.17668291926383972 \n",
      "iterator 1070 : loss 0.17552199959754944 \n",
      "iterator 1080 : loss 0.18302032351493835 \n",
      "iterator 1090 : loss 0.1879304051399231 \n",
      "iterator 1100 : loss 0.18059776723384857 \n",
      "iterator 1110 : loss 0.1905270367860794 \n",
      "iterator 1120 : loss 0.18937243521213531 \n",
      "iterator 1130 : loss 0.17179043591022491 \n",
      "iterator 1140 : loss 0.1614566296339035 \n",
      "iterator 1150 : loss 0.17703120410442352 \n",
      "iterator 1160 : loss 0.18510957062244415 \n",
      "iterator 1170 : loss 0.1556335985660553 \n",
      "iterator 1180 : loss 0.16490720212459564 \n",
      "iterator 1190 : loss 0.16405494511127472 \n",
      "iterator 1200 : loss 0.16814765334129333 \n",
      "iterator 1210 : loss 0.17307277023792267 \n",
      "iterator 1220 : loss 0.1858833283185959 \n",
      "iterator 1230 : loss 0.17118729650974274 \n",
      "iterator 1240 : loss 0.17439091205596924 \n",
      "iterator 1250 : loss 0.17195263504981995 \n",
      "iterator 1260 : loss 0.16133567690849304 \n",
      "iterator 1270 : loss 0.1736452579498291 \n",
      "iterator 1280 : loss 0.16426923871040344 \n",
      "iterator 1290 : loss 0.1597708761692047 \n",
      "iterator 1300 : loss 0.16350851953029633 \n",
      "iterator 1310 : loss 0.1675003468990326 \n",
      "iterator 1320 : loss 0.1689871996641159 \n",
      "iterator 1330 : loss 0.15456540882587433 \n",
      "iterator 1340 : loss 0.16667136549949646 \n",
      "iterator 1350 : loss 0.15733911097049713 \n",
      "iterator 1360 : loss 0.16234412789344788 \n",
      "iterator 1370 : loss 0.14283958077430725 \n",
      "iterator 1380 : loss 0.15600937604904175 \n",
      "iterator 1390 : loss 0.16025318205356598 \n",
      "iterator 1400 : loss 0.1640785038471222 \n",
      "iterator 1410 : loss 0.16251641511917114 \n",
      "iterator 1420 : loss 0.16270387172698975 \n",
      "iterator 1430 : loss 0.150429829955101 \n",
      "iterator 1440 : loss 0.17071533203125 \n",
      "iterator 1450 : loss 0.16726867854595184 \n",
      "iterator 1460 : loss 0.15406915545463562 \n",
      "iterator 1470 : loss 0.1576385647058487 \n",
      "iterator 1480 : loss 0.16380387544631958 \n",
      "iterator 1490 : loss 0.15628263354301453 \n",
      "iterator 1500 : loss 0.17115585505962372 \n",
      "iterator 1510 : loss 0.17442664504051208 \n",
      "iterator 1520 : loss 0.16335120797157288 \n",
      "iterator 1530 : loss 0.15268515050411224 \n",
      "iterator 1540 : loss 0.16737963259220123 \n",
      "iterator 1550 : loss 0.16039922833442688 \n",
      "iterator 1560 : loss 0.16362687945365906 \n",
      "iterator 1570 : loss 0.1605229526758194 \n",
      "iterator 1580 : loss 0.16735108196735382 \n",
      "iterator 1590 : loss 0.15892541408538818 \n",
      "iterator 1600 : loss 0.1561901569366455 \n",
      "iterator 1610 : loss 0.14198118448257446 \n",
      "iterator 1620 : loss 0.1515558362007141 \n",
      "iterator 1630 : loss 0.15207412838935852 \n",
      "iterator 1640 : loss 0.14840152859687805 \n",
      "iterator 1650 : loss 0.15460491180419922 \n",
      "iterator 1660 : loss 0.15840017795562744 \n",
      "iterator 1670 : loss 0.14248092472553253 \n",
      "iterator 1680 : loss 0.1563466191291809 \n",
      "iterator 1690 : loss 0.1621086597442627 \n",
      "iterator 1700 : loss 0.16080142557621002 \n",
      "iterator 1710 : loss 0.15773166716098785 \n",
      "iterator 1720 : loss 0.15555037558078766 \n",
      "iterator 1730 : loss 0.1581510305404663 \n",
      "iterator 1740 : loss 0.1501195728778839 \n",
      "iterator 1750 : loss 0.1592450886964798 \n",
      "iterator 1760 : loss 0.13966260850429535 \n",
      "iterator 1770 : loss 0.15741324424743652 \n",
      "iterator 1780 : loss 0.15705877542495728 \n",
      "iterator 1790 : loss 0.16242970526218414 \n",
      "iterator 1800 : loss 0.15279993414878845 \n",
      "iterator 1810 : loss 0.14494408667087555 \n",
      "iterator 1820 : loss 0.1402857005596161 \n",
      "iterator 1830 : loss 0.1483316570520401 \n",
      "iterator 1840 : loss 0.14781486988067627 \n",
      "iterator 1850 : loss 0.1487766057252884 \n",
      "iterator 1860 : loss 0.13973122835159302 \n",
      "iterator 1870 : loss 0.15464577078819275 \n",
      "iterator 1880 : loss 0.14988577365875244 \n",
      "iterator 1890 : loss 0.14813929796218872 \n",
      "iterator 1900 : loss 0.15038225054740906 \n",
      "iterator 1910 : loss 0.15666700899600983 \n",
      "iterator 1920 : loss 0.1515912264585495 \n",
      "iterator 1930 : loss 0.13602623343467712 \n",
      "iterator 1940 : loss 0.14359991252422333 \n",
      "iterator 1950 : loss 0.1533140391111374 \n",
      "iterator 1960 : loss 0.14879901707172394 \n",
      "iterator 1970 : loss 0.15396733582019806 \n",
      "iterator 1980 : loss 0.16238543391227722 \n",
      "iterator 1990 : loss 0.1395205408334732 \n",
      "iterator 2000 : loss 0.1529274880886078 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 2010 : loss 0.14918901026248932 \n",
      "iterator 2020 : loss 0.14822612702846527 \n",
      "iterator 2030 : loss 0.1606767475605011 \n",
      "iterator 2040 : loss 0.14703369140625 \n",
      "iterator 2050 : loss 0.16682842373847961 \n",
      "iterator 2060 : loss 0.14742453396320343 \n",
      "iterator 2070 : loss 0.1341942399740219 \n",
      "iterator 2080 : loss 0.14493127167224884 \n",
      "iterator 2090 : loss 0.14577022194862366 \n",
      "iterator 2100 : loss 0.14521998167037964 \n",
      "iterator 2110 : loss 0.14358897507190704 \n",
      "iterator 2120 : loss 0.1473180651664734 \n",
      "iterator 2130 : loss 0.16213902831077576 \n",
      "iterator 2140 : loss 0.13965243101119995 \n",
      "iterator 2150 : loss 0.14628322422504425 \n",
      "iterator 2160 : loss 0.143411323428154 \n",
      "iterator 2170 : loss 0.15607595443725586 \n",
      "iterator 2180 : loss 0.14491452276706696 \n",
      "iterator 2190 : loss 0.1500318944454193 \n",
      "iterator 2200 : loss 0.13806535303592682 \n",
      "iterator 2210 : loss 0.14882664382457733 \n",
      "iterator 2220 : loss 0.15213941037654877 \n",
      "iterator 2230 : loss 0.14527752995491028 \n",
      "iterator 2240 : loss 0.1402617245912552 \n",
      "iterator 2250 : loss 0.14468638598918915 \n",
      "iterator 2260 : loss 0.1567276418209076 \n",
      "iterator 2270 : loss 0.1394663006067276 \n",
      "iterator 2280 : loss 0.13201692700386047 \n",
      "iterator 2290 : loss 0.14417287707328796 \n",
      "iterator 2300 : loss 0.14150109887123108 \n",
      "iterator 2310 : loss 0.13666683435440063 \n",
      "iterator 2320 : loss 0.1504102200269699 \n",
      "iterator 2330 : loss 0.14117668569087982 \n",
      "iterator 2340 : loss 0.15477308630943298 \n",
      "iterator 2350 : loss 0.1457376331090927 \n",
      "iterator 2360 : loss 0.14886628091335297 \n",
      "iterator 2370 : loss 0.1417011320590973 \n",
      "iterator 2380 : loss 0.13896778225898743 \n",
      "iterator 2390 : loss 0.1469254046678543 \n",
      "iterator 2400 : loss 0.1425190418958664 \n",
      "iterator 2410 : loss 0.14295007288455963 \n",
      "iterator 2420 : loss 0.15999029576778412 \n",
      "iterator 2430 : loss 0.13770179450511932 \n",
      "iterator 2440 : loss 0.15417762100696564 \n",
      "iterator 2450 : loss 0.15472017228603363 \n",
      "iterator 2460 : loss 0.13868092000484467 \n",
      "iterator 2470 : loss 0.14010734856128693 \n",
      "iterator 2480 : loss 0.1507256031036377 \n",
      "iterator 2490 : loss 0.13322879374027252 \n",
      "iterator 2500 : loss 0.1500750035047531 \n",
      "iterator 2510 : loss 0.1474776715040207 \n",
      "iterator 2520 : loss 0.16732043027877808 \n",
      "iterator 2530 : loss 0.13902290165424347 \n",
      "iterator 2540 : loss 0.13263195753097534 \n",
      "iterator 2550 : loss 0.1440238207578659 \n",
      "iterator 2560 : loss 0.1363091915845871 \n",
      "iterator 2570 : loss 0.14999794960021973 \n",
      "iterator 2580 : loss 0.14039255678653717 \n",
      "iterator 2590 : loss 0.13212937116622925 \n",
      "iterator 2600 : loss 0.14930954575538635 \n",
      "iterator 2610 : loss 0.13331443071365356 \n",
      "iterator 2620 : loss 0.13964636623859406 \n",
      "iterator 2630 : loss 0.1458313763141632 \n",
      "iterator 2640 : loss 0.14883345365524292 \n",
      "iterator 2650 : loss 0.1377887725830078 \n",
      "iterator 2660 : loss 0.1464434713125229 \n",
      "iterator 2670 : loss 0.14216677844524384 \n",
      "iterator 2680 : loss 0.13648919761180878 \n",
      "iterator 2690 : loss 0.13355910778045654 \n",
      "iterator 2700 : loss 0.1396857053041458 \n",
      "iterator 2710 : loss 0.13762502372264862 \n",
      "iterator 2720 : loss 0.1481391042470932 \n",
      "iterator 2730 : loss 0.13837815821170807 \n",
      "iterator 2740 : loss 0.13496406376361847 \n",
      "iterator 2750 : loss 0.13903607428073883 \n",
      "iterator 2760 : loss 0.12780947983264923 \n",
      "iterator 2770 : loss 0.13104520738124847 \n",
      "iterator 2780 : loss 0.14044280350208282 \n",
      "iterator 2790 : loss 0.14606355130672455 \n",
      "iterator 2800 : loss 0.12553811073303223 \n",
      "iterator 2810 : loss 0.15200968086719513 \n",
      "iterator 2820 : loss 0.1424252986907959 \n",
      "iterator 2830 : loss 0.1371888965368271 \n",
      "iterator 2840 : loss 0.12665556371212006 \n",
      "iterator 2850 : loss 0.14242036640644073 \n",
      "iterator 2860 : loss 0.14237764477729797 \n",
      "iterator 2870 : loss 0.1450127363204956 \n",
      "iterator 2880 : loss 0.1388368010520935 \n",
      "iterator 2890 : loss 0.14397037029266357 \n",
      "iterator 2900 : loss 0.13215607404708862 \n",
      "iterator 2910 : loss 0.14285388588905334 \n",
      "iterator 2920 : loss 0.1434224545955658 \n",
      "iterator 2930 : loss 0.14073629677295685 \n",
      "iterator 2940 : loss 0.13386720418930054 \n",
      "iterator 2950 : loss 0.1421656459569931 \n",
      "iterator 2960 : loss 0.13995563983917236 \n",
      "iterator 2970 : loss 0.14086049795150757 \n",
      "iterator 2980 : loss 0.1410388946533203 \n",
      "iterator 2990 : loss 0.14447623491287231 \n",
      "iterator 3000 : loss 0.15031218528747559 \n",
      "iterator 3010 : loss 0.13159047067165375 \n",
      "iterator 3020 : loss 0.13107888400554657 \n",
      "iterator 3030 : loss 0.14568842947483063 \n",
      "iterator 3040 : loss 0.12996704876422882 \n",
      "iterator 3050 : loss 0.12935799360275269 \n",
      "iterator 3060 : loss 0.13634362816810608 \n",
      "iterator 3070 : loss 0.1382700502872467 \n",
      "iterator 3080 : loss 0.1332094371318817 \n",
      "iterator 3090 : loss 0.1376056671142578 \n",
      "iterator 3100 : loss 0.14179228246212006 \n",
      "iterator 3110 : loss 0.13654036819934845 \n",
      "iterator 3120 : loss 0.135952427983284 \n",
      "iterator 3130 : loss 0.1374867856502533 \n",
      "iterator 3140 : loss 0.13208438456058502 \n",
      "iterator 3150 : loss 0.13210079073905945 \n",
      "iterator 3160 : loss 0.15091776847839355 \n",
      "iterator 3170 : loss 0.13299432396888733 \n",
      "iterator 3180 : loss 0.1393561214208603 \n",
      "iterator 3190 : loss 0.131122887134552 \n",
      "iterator 3200 : loss 0.13399195671081543 \n",
      "iterator 3210 : loss 0.144211083650589 \n",
      "iterator 3220 : loss 0.12757538259029388 \n",
      "iterator 3230 : loss 0.13629081845283508 \n",
      "iterator 3240 : loss 0.12668804824352264 \n",
      "iterator 3250 : loss 0.13484801352024078 \n",
      "iterator 3260 : loss 0.1259099543094635 \n",
      "iterator 3270 : loss 0.1256544291973114 \n",
      "iterator 3280 : loss 0.14301110804080963 \n",
      "iterator 3290 : loss 0.13748832046985626 \n",
      "iterator 3300 : loss 0.137327179312706 \n",
      "iterator 3310 : loss 0.1297188401222229 \n",
      "iterator 3320 : loss 0.1346605122089386 \n",
      "iterator 3330 : loss 0.13245829939842224 \n",
      "iterator 3340 : loss 0.1366003453731537 \n",
      "iterator 3350 : loss 0.1340886801481247 \n",
      "iterator 3360 : loss 0.14781776070594788 \n",
      "iterator 3370 : loss 0.12752337753772736 \n",
      "iterator 3380 : loss 0.14895018935203552 \n",
      "iterator 3390 : loss 0.1437508463859558 \n",
      "iterator 3400 : loss 0.12512801587581635 \n",
      "iterator 3410 : loss 0.14120160043239594 \n",
      "iterator 3420 : loss 0.13183055818080902 \n",
      "iterator 3430 : loss 0.14180196821689606 \n",
      "iterator 3440 : loss 0.14536583423614502 \n",
      "iterator 3450 : loss 0.1329563856124878 \n",
      "iterator 3460 : loss 0.13680483400821686 \n",
      "iterator 3470 : loss 0.13023316860198975 \n",
      "iterator 3480 : loss 0.13160288333892822 \n",
      "iterator 3490 : loss 0.1284109652042389 \n",
      "iterator 3500 : loss 0.13881231844425201 \n",
      "iterator 3510 : loss 0.12747430801391602 \n",
      "iterator 3520 : loss 0.12162866443395615 \n",
      "iterator 3530 : loss 0.1390366107225418 \n",
      "iterator 3540 : loss 0.12925444543361664 \n",
      "iterator 3550 : loss 0.1329299360513687 \n",
      "iterator 3560 : loss 0.13305242359638214 \n",
      "iterator 3570 : loss 0.1375933736562729 \n",
      "iterator 3580 : loss 0.1340027004480362 \n",
      "iterator 3590 : loss 0.1399085521697998 \n",
      "iterator 3600 : loss 0.14199914038181305 \n",
      "iterator 3610 : loss 0.13842755556106567 \n",
      "iterator 3620 : loss 0.12698747217655182 \n",
      "iterator 3630 : loss 0.13039901852607727 \n",
      "iterator 3640 : loss 0.12971743941307068 \n",
      "iterator 3650 : loss 0.13804712891578674 \n",
      "iterator 3660 : loss 0.13997790217399597 \n",
      "iterator 3670 : loss 0.12818197906017303 \n",
      "iterator 3680 : loss 0.12676741182804108 \n",
      "iterator 3690 : loss 0.13309039175510406 \n",
      "iterator 3700 : loss 0.1392255276441574 \n",
      "iterator 3710 : loss 0.12489251047372818 \n",
      "iterator 3720 : loss 0.1414060741662979 \n",
      "iterator 3730 : loss 0.13386031985282898 \n",
      "iterator 3740 : loss 0.1243719831109047 \n",
      "iterator 3750 : loss 0.12465235590934753 \n",
      "iterator 3760 : loss 0.1301192045211792 \n",
      "iterator 3770 : loss 0.13528287410736084 \n",
      "iterator 3780 : loss 0.1278115063905716 \n",
      "iterator 3790 : loss 0.1348785161972046 \n",
      "iterator 3800 : loss 0.13078638911247253 \n",
      "iterator 3810 : loss 0.13379552960395813 \n",
      "iterator 3820 : loss 0.13180699944496155 \n",
      "iterator 3830 : loss 0.12774573266506195 \n",
      "iterator 3840 : loss 0.13279655575752258 \n",
      "iterator 3850 : loss 0.13669539988040924 \n",
      "iterator 3860 : loss 0.14331167936325073 \n",
      "iterator 3870 : loss 0.13139770925045013 \n",
      "iterator 3880 : loss 0.13031916320323944 \n",
      "iterator 3890 : loss 0.12416039407253265 \n",
      "iterator 3900 : loss 0.1395586133003235 \n",
      "iterator 3910 : loss 0.13403649628162384 \n",
      "iterator 3920 : loss 0.13009339570999146 \n",
      "iterator 3930 : loss 0.1382814347743988 \n",
      "iterator 3940 : loss 0.12389589846134186 \n",
      "iterator 3950 : loss 0.11691601574420929 \n",
      "iterator 3960 : loss 0.13391521573066711 \n",
      "iterator 3970 : loss 0.13120922446250916 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 3980 : loss 0.13246619701385498 \n",
      "iterator 3990 : loss 0.12074305862188339 \n",
      "iterator 4000 : loss 0.13524384796619415 \n",
      "iterator 4010 : loss 0.12944677472114563 \n",
      "iterator 4020 : loss 0.12863346934318542 \n",
      "iterator 4030 : loss 0.13571269810199738 \n",
      "iterator 4040 : loss 0.1281278431415558 \n",
      "iterator 4050 : loss 0.13182508945465088 \n",
      "iterator 4060 : loss 0.13654553890228271 \n",
      "iterator 4070 : loss 0.12878629565238953 \n",
      "iterator 4080 : loss 0.13750584423542023 \n",
      "iterator 4090 : loss 0.11933557689189911 \n",
      "iterator 4100 : loss 0.12171217799186707 \n",
      "iterator 4110 : loss 0.13930641114711761 \n",
      "iterator 4120 : loss 0.12910418212413788 \n",
      "iterator 4130 : loss 0.13938060402870178 \n",
      "iterator 4140 : loss 0.1363215148448944 \n",
      "iterator 4150 : loss 0.13203902542591095 \n",
      "iterator 4160 : loss 0.12889495491981506 \n",
      "iterator 4170 : loss 0.12877202033996582 \n",
      "iterator 4180 : loss 0.126073956489563 \n",
      "iterator 4190 : loss 0.126180499792099 \n",
      "iterator 4200 : loss 0.127607062458992 \n",
      "iterator 4210 : loss 0.14860612154006958 \n",
      "iterator 4220 : loss 0.13042764365673065 \n",
      "iterator 4230 : loss 0.1293180286884308 \n",
      "iterator 4240 : loss 0.12737390398979187 \n",
      "iterator 4250 : loss 0.130910724401474 \n",
      "iterator 4260 : loss 0.12789751589298248 \n",
      "iterator 4270 : loss 0.12454256415367126 \n",
      "iterator 4280 : loss 0.1284329891204834 \n",
      "iterator 4290 : loss 0.14922092854976654 \n",
      "iterator 4300 : loss 0.12322914600372314 \n",
      "iterator 4310 : loss 0.1248520240187645 \n",
      "iterator 4320 : loss 0.13293342292308807 \n",
      "iterator 4330 : loss 0.14387552440166473 \n",
      "iterator 4340 : loss 0.13236761093139648 \n",
      "iterator 4350 : loss 0.13740047812461853 \n",
      "iterator 4360 : loss 0.11824003607034683 \n",
      "iterator 4370 : loss 0.13568444550037384 \n",
      "iterator 4380 : loss 0.12562593817710876 \n",
      "iterator 4390 : loss 0.12629663944244385 \n",
      "iterator 4400 : loss 0.12891170382499695 \n",
      "iterator 4410 : loss 0.1324375718832016 \n",
      "iterator 4420 : loss 0.11607431620359421 \n",
      "iterator 4430 : loss 0.13165497779846191 \n",
      "iterator 4440 : loss 0.14029629528522491 \n",
      "iterator 4450 : loss 0.1289382427930832 \n",
      "iterator 4460 : loss 0.11898615956306458 \n",
      "iterator 4470 : loss 0.13220277428627014 \n",
      "iterator 4480 : loss 0.1220841333270073 \n",
      "iterator 4490 : loss 0.12453348934650421 \n",
      "iterator 4500 : loss 0.1350547969341278 \n",
      "iterator 4510 : loss 0.13221468031406403 \n",
      "iterator 4520 : loss 0.12358604371547699 \n",
      "iterator 4530 : loss 0.12592561542987823 \n",
      "iterator 4540 : loss 0.12931494414806366 \n",
      "iterator 4550 : loss 0.13369764387607574 \n",
      "iterator 4560 : loss 0.12319023907184601 \n",
      "iterator 4570 : loss 0.12929844856262207 \n",
      "iterator 4580 : loss 0.1297021061182022 \n",
      "iterator 4590 : loss 0.12332677096128464 \n",
      "iterator 4600 : loss 0.13123025000095367 \n",
      "iterator 4610 : loss 0.12897808849811554 \n",
      "iterator 4620 : loss 0.13625405728816986 \n",
      "iterator 4630 : loss 0.12143278121948242 \n",
      "iterator 4640 : loss 0.11772193014621735 \n",
      "iterator 4650 : loss 0.12080587446689606 \n",
      "iterator 4660 : loss 0.11927524954080582 \n",
      "iterator 4670 : loss 0.11226744204759598 \n",
      "iterator 4680 : loss 0.13491037487983704 \n",
      "iterator 4690 : loss 0.12139390408992767 \n",
      "iterator 4700 : loss 0.1289217621088028 \n",
      "iterator 4710 : loss 0.11655270308256149 \n",
      "iterator 4720 : loss 0.13085782527923584 \n",
      "iterator 4730 : loss 0.12335570901632309 \n",
      "iterator 4740 : loss 0.12286002933979034 \n",
      "iterator 4750 : loss 0.12487779557704926 \n",
      "iterator 4760 : loss 0.1349308341741562 \n",
      "iterator 4770 : loss 0.12213243544101715 \n",
      "iterator 4780 : loss 0.13158001005649567 \n",
      "iterator 4790 : loss 0.13298802077770233 \n",
      "iterator 4800 : loss 0.12950752675533295 \n",
      "iterator 4810 : loss 0.1295839101076126 \n",
      "iterator 4820 : loss 0.12625987827777863 \n",
      "iterator 4830 : loss 0.1243426576256752 \n",
      "iterator 4840 : loss 0.1470547765493393 \n",
      "iterator 4850 : loss 0.12686826288700104 \n",
      "iterator 4860 : loss 0.13835842907428741 \n",
      "iterator 4870 : loss 0.12441590428352356 \n",
      "iterator 4880 : loss 0.1176934763789177 \n",
      "iterator 4890 : loss 0.12322399765253067 \n",
      "iterator 4900 : loss 0.13262797892093658 \n",
      "iterator 4910 : loss 0.128498837351799 \n",
      "iterator 4920 : loss 0.12351452559232712 \n",
      "iterator 4930 : loss 0.12661431729793549 \n",
      "iterator 4940 : loss 0.13293419778347015 \n",
      "iterator 4950 : loss 0.11847817152738571 \n",
      "iterator 4960 : loss 0.11767607182264328 \n",
      "iterator 4970 : loss 0.13182276487350464 \n",
      "iterator 4980 : loss 0.1319597214460373 \n",
      "iterator 4990 : loss 0.12181593477725983 \n",
      "iterator 5000 : loss 0.12536847591400146 \n",
      "iterator 5010 : loss 0.12149570137262344 \n",
      "iterator 5020 : loss 0.12047074735164642 \n",
      "iterator 5030 : loss 0.12104187160730362 \n",
      "iterator 5040 : loss 0.1270783245563507 \n",
      "iterator 5050 : loss 0.12522895634174347 \n",
      "iterator 5060 : loss 0.13370591402053833 \n",
      "iterator 5070 : loss 0.13639245927333832 \n",
      "iterator 5080 : loss 0.1296873837709427 \n",
      "iterator 5090 : loss 0.12139816582202911 \n",
      "iterator 5100 : loss 0.1253081113100052 \n",
      "iterator 5110 : loss 0.1195601224899292 \n",
      "iterator 5120 : loss 0.11763786524534225 \n",
      "iterator 5130 : loss 0.11567948758602142 \n",
      "iterator 5140 : loss 0.13581068813800812 \n",
      "iterator 5150 : loss 0.1265718638896942 \n",
      "iterator 5160 : loss 0.12133761495351791 \n",
      "iterator 5170 : loss 0.11907822638750076 \n",
      "iterator 5180 : loss 0.12143318355083466 \n",
      "iterator 5190 : loss 0.13354308903217316 \n",
      "iterator 5200 : loss 0.13250742852687836 \n",
      "iterator 5210 : loss 0.11610184609889984 \n",
      "iterator 5220 : loss 0.13810837268829346 \n",
      "iterator 5230 : loss 0.12610410153865814 \n",
      "iterator 5240 : loss 0.11310582607984543 \n",
      "iterator 5250 : loss 0.13208118081092834 \n",
      "iterator 5260 : loss 0.12101143598556519 \n",
      "iterator 5270 : loss 0.13026247918605804 \n",
      "iterator 5280 : loss 0.12364469468593597 \n",
      "iterator 5290 : loss 0.12696954607963562 \n",
      "iterator 5300 : loss 0.12352443486452103 \n",
      "iterator 5310 : loss 0.13399554789066315 \n",
      "iterator 5320 : loss 0.12716764211654663 \n",
      "iterator 5330 : loss 0.12648645043373108 \n",
      "iterator 5340 : loss 0.13373354077339172 \n",
      "iterator 5350 : loss 0.11184930801391602 \n",
      "iterator 5360 : loss 0.11697228252887726 \n",
      "iterator 5370 : loss 0.1260858029127121 \n",
      "iterator 5380 : loss 0.13172350823879242 \n",
      "iterator 5390 : loss 0.1263977587223053 \n",
      "iterator 5400 : loss 0.1202877014875412 \n",
      "iterator 5410 : loss 0.11988867819309235 \n",
      "iterator 5420 : loss 0.12021280825138092 \n",
      "iterator 5430 : loss 0.12197418510913849 \n",
      "iterator 5440 : loss 0.12257932126522064 \n",
      "iterator 5450 : loss 0.12413085252046585 \n",
      "iterator 5460 : loss 0.11714788526296616 \n",
      "iterator 5470 : loss 0.11616097390651703 \n",
      "iterator 5480 : loss 0.12868034839630127 \n",
      "iterator 5490 : loss 0.1235576942563057 \n",
      "iterator 5500 : loss 0.1175621747970581 \n",
      "iterator 5510 : loss 0.1214548796415329 \n",
      "iterator 5520 : loss 0.12847794592380524 \n",
      "iterator 5530 : loss 0.13101577758789062 \n",
      "iterator 5540 : loss 0.13003218173980713 \n",
      "iterator 5550 : loss 0.1196805089712143 \n",
      "iterator 5560 : loss 0.1265045702457428 \n",
      "iterator 5570 : loss 0.12821051478385925 \n",
      "iterator 5580 : loss 0.11707506328821182 \n",
      "iterator 5590 : loss 0.11728528141975403 \n",
      "iterator 5600 : loss 0.11662661284208298 \n",
      "iterator 5610 : loss 0.11760425567626953 \n",
      "iterator 5620 : loss 0.1279422491788864 \n",
      "iterator 5630 : loss 0.12222613394260406 \n",
      "iterator 5640 : loss 0.11857551336288452 \n",
      "iterator 5650 : loss 0.1213824599981308 \n",
      "iterator 5660 : loss 0.12547782063484192 \n",
      "iterator 5670 : loss 0.12413635849952698 \n",
      "iterator 5680 : loss 0.12413876503705978 \n",
      "iterator 5690 : loss 0.12387418746948242 \n",
      "iterator 5700 : loss 0.125260591506958 \n",
      "iterator 5710 : loss 0.11644244939088821 \n",
      "iterator 5720 : loss 0.12815502285957336 \n",
      "iterator 5730 : loss 0.13182489573955536 \n",
      "iterator 5740 : loss 0.12150872498750687 \n",
      "iterator 5750 : loss 0.11532460153102875 \n",
      "iterator 5760 : loss 0.12179464846849442 \n",
      "iterator 5770 : loss 0.12896879017353058 \n",
      "iterator 5780 : loss 0.13652803003787994 \n",
      "iterator 5790 : loss 0.12216977775096893 \n",
      "iterator 5800 : loss 0.12866155803203583 \n",
      "iterator 5810 : loss 0.12286104261875153 \n",
      "iterator 5820 : loss 0.11013370007276535 \n",
      "iterator 5830 : loss 0.125494122505188 \n",
      "iterator 5840 : loss 0.1161186620593071 \n",
      "iterator 5850 : loss 0.12136710435152054 \n",
      "iterator 5860 : loss 0.12600034475326538 \n",
      "iterator 5870 : loss 0.11897246539592743 \n",
      "iterator 5880 : loss 0.1228107362985611 \n",
      "iterator 5890 : loss 0.1180262491106987 \n",
      "iterator 5900 : loss 0.11199042201042175 \n",
      "iterator 5910 : loss 0.12038262188434601 \n",
      "iterator 5920 : loss 0.12218974530696869 \n",
      "iterator 5930 : loss 0.12050720304250717 \n",
      "iterator 5940 : loss 0.12526485323905945 \n",
      "iterator 5950 : loss 0.1148901954293251 \n",
      "iterator 5960 : loss 0.11603116244077682 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 5970 : loss 0.12288141995668411 \n",
      "iterator 5980 : loss 0.12223870307207108 \n",
      "iterator 5990 : loss 0.11555340886116028 \n",
      "iterator 6000 : loss 0.13283607363700867 \n",
      "iterator 6010 : loss 0.12716159224510193 \n",
      "iterator 6020 : loss 0.12069634348154068 \n",
      "iterator 6030 : loss 0.1171937957406044 \n",
      "iterator 6040 : loss 0.11146186292171478 \n",
      "iterator 6050 : loss 0.120390385389328 \n",
      "iterator 6060 : loss 0.12057734280824661 \n",
      "iterator 6070 : loss 0.12542764842510223 \n",
      "iterator 6080 : loss 0.10857763141393661 \n",
      "iterator 6090 : loss 0.13171467185020447 \n",
      "iterator 6100 : loss 0.12154445797204971 \n",
      "iterator 6110 : loss 0.1259518712759018 \n",
      "iterator 6120 : loss 0.1196584701538086 \n",
      "iterator 6130 : loss 0.12732866406440735 \n",
      "iterator 6140 : loss 0.12229756265878677 \n",
      "iterator 6150 : loss 0.1118578240275383 \n",
      "iterator 6160 : loss 0.13672921061515808 \n",
      "iterator 6170 : loss 0.12777268886566162 \n",
      "iterator 6180 : loss 0.12083305418491364 \n",
      "iterator 6190 : loss 0.12911422550678253 \n",
      "iterator 6200 : loss 0.13440966606140137 \n",
      "iterator 6210 : loss 0.11844936013221741 \n",
      "iterator 6220 : loss 0.1223142221570015 \n",
      "iterator 6230 : loss 0.11971043795347214 \n",
      "iterator 6240 : loss 0.1203221008181572 \n",
      "iterator 6250 : loss 0.12502256035804749 \n",
      "iterator 6260 : loss 0.11361939460039139 \n",
      "iterator 6270 : loss 0.1323237270116806 \n",
      "iterator 6280 : loss 0.1168915405869484 \n",
      "iterator 6290 : loss 0.11489102244377136 \n",
      "iterator 6300 : loss 0.11515156179666519 \n",
      "iterator 6310 : loss 0.1170184388756752 \n",
      "iterator 6320 : loss 0.1183679923415184 \n",
      "iterator 6330 : loss 0.11304325610399246 \n",
      "iterator 6340 : loss 0.11633173376321793 \n",
      "iterator 6350 : loss 0.11013027280569077 \n",
      "iterator 6360 : loss 0.1190631240606308 \n",
      "iterator 6370 : loss 0.12434515357017517 \n",
      "iterator 6380 : loss 0.13235220313072205 \n",
      "iterator 6390 : loss 0.12026278674602509 \n",
      "iterator 6400 : loss 0.12050531059503555 \n",
      "iterator 6410 : loss 0.12573255598545074 \n",
      "iterator 6420 : loss 0.12470795959234238 \n",
      "iterator 6430 : loss 0.12516619265079498 \n",
      "iterator 6440 : loss 0.12069663405418396 \n",
      "iterator 6450 : loss 0.12099218368530273 \n",
      "iterator 6460 : loss 0.12695857882499695 \n",
      "iterator 6470 : loss 0.12901639938354492 \n",
      "iterator 6480 : loss 0.1253117173910141 \n",
      "iterator 6490 : loss 0.111577607691288 \n",
      "iterator 6500 : loss 0.12083110213279724 \n",
      "iterator 6510 : loss 0.1147012934088707 \n",
      "iterator 6520 : loss 0.11691319197416306 \n",
      "iterator 6530 : loss 0.11878111213445663 \n",
      "iterator 6540 : loss 0.1199580579996109 \n",
      "iterator 6550 : loss 0.11325536668300629 \n",
      "iterator 6560 : loss 0.12119553983211517 \n",
      "iterator 6570 : loss 0.11851446330547333 \n",
      "iterator 6580 : loss 0.11918456852436066 \n",
      "iterator 6590 : loss 0.10958090424537659 \n",
      "iterator 6600 : loss 0.12205196171998978 \n",
      "iterator 6610 : loss 0.1166011169552803 \n",
      "iterator 6620 : loss 0.11423157900571823 \n",
      "iterator 6630 : loss 0.1282738745212555 \n",
      "iterator 6640 : loss 0.11839329451322556 \n",
      "iterator 6650 : loss 0.11495574563741684 \n",
      "iterator 6660 : loss 0.12260717153549194 \n",
      "iterator 6670 : loss 0.12206678092479706 \n",
      "iterator 6680 : loss 0.12007392197847366 \n",
      "iterator 6690 : loss 0.12437715381383896 \n",
      "iterator 6700 : loss 0.1208769902586937 \n",
      "iterator 6710 : loss 0.1207844465970993 \n",
      "iterator 6720 : loss 0.11445332318544388 \n",
      "iterator 6730 : loss 0.11749156564474106 \n",
      "iterator 6740 : loss 0.1261061131954193 \n",
      "iterator 6750 : loss 0.11748045682907104 \n",
      "iterator 6760 : loss 0.11073678731918335 \n",
      "iterator 6770 : loss 0.11939098685979843 \n",
      "iterator 6780 : loss 0.12716253101825714 \n",
      "iterator 6790 : loss 0.11304644495248795 \n",
      "iterator 6800 : loss 0.11675433814525604 \n",
      "iterator 6810 : loss 0.11768156290054321 \n",
      "iterator 6820 : loss 0.1134861409664154 \n",
      "iterator 6830 : loss 0.11325967311859131 \n",
      "iterator 6840 : loss 0.11019395291805267 \n",
      "iterator 6850 : loss 0.1240871474146843 \n",
      "iterator 6860 : loss 0.12510836124420166 \n",
      "iterator 6870 : loss 0.11509500443935394 \n",
      "iterator 6880 : loss 0.12268776446580887 \n",
      "iterator 6890 : loss 0.11598604917526245 \n",
      "iterator 6900 : loss 0.117159403860569 \n",
      "iterator 6910 : loss 0.11480948328971863 \n",
      "iterator 6920 : loss 0.12127414345741272 \n",
      "iterator 6930 : loss 0.1294252574443817 \n",
      "iterator 6940 : loss 0.115630604326725 \n",
      "iterator 6950 : loss 0.1258210986852646 \n",
      "iterator 6960 : loss 0.10986112803220749 \n",
      "iterator 6970 : loss 0.11903661489486694 \n",
      "iterator 6980 : loss 0.1235831007361412 \n",
      "iterator 6990 : loss 0.12001152336597443 \n",
      "iterator 7000 : loss 0.11944907158613205 \n",
      "iterator 7010 : loss 0.11521555483341217 \n",
      "iterator 7020 : loss 0.1183844804763794 \n",
      "iterator 7030 : loss 0.11742076277732849 \n",
      "iterator 7040 : loss 0.12412379682064056 \n",
      "iterator 7050 : loss 0.1171150952577591 \n",
      "iterator 7060 : loss 0.1042846143245697 \n",
      "iterator 7070 : loss 0.11865123361349106 \n",
      "iterator 7080 : loss 0.11541187763214111 \n",
      "iterator 7090 : loss 0.13172577321529388 \n",
      "iterator 7100 : loss 0.12044869363307953 \n",
      "iterator 7110 : loss 0.12278442829847336 \n",
      "iterator 7120 : loss 0.11387752741575241 \n",
      "iterator 7130 : loss 0.12963217496871948 \n",
      "iterator 7140 : loss 0.12252581864595413 \n",
      "iterator 7150 : loss 0.12887439131736755 \n",
      "iterator 7160 : loss 0.11729943007230759 \n",
      "iterator 7170 : loss 0.12111426144838333 \n",
      "iterator 7180 : loss 0.13347750902175903 \n",
      "iterator 7190 : loss 0.12224213778972626 \n",
      "iterator 7200 : loss 0.13405826687812805 \n",
      "iterator 7210 : loss 0.13089680671691895 \n",
      "iterator 7220 : loss 0.10985158383846283 \n",
      "iterator 7230 : loss 0.10816241055727005 \n",
      "iterator 7240 : loss 0.11188720911741257 \n",
      "iterator 7250 : loss 0.12333778291940689 \n",
      "iterator 7260 : loss 0.11185454577207565 \n",
      "iterator 7270 : loss 0.10996119678020477 \n",
      "iterator 7280 : loss 0.12336273491382599 \n",
      "iterator 7290 : loss 0.1222287267446518 \n",
      "iterator 7300 : loss 0.12235252559185028 \n",
      "iterator 7310 : loss 0.1204795315861702 \n",
      "iterator 7320 : loss 0.125636026263237 \n",
      "iterator 7330 : loss 0.1178155466914177 \n",
      "iterator 7340 : loss 0.11557304859161377 \n",
      "iterator 7350 : loss 0.12062596529722214 \n",
      "iterator 7360 : loss 0.12967310845851898 \n",
      "iterator 7370 : loss 0.11732073873281479 \n",
      "iterator 7380 : loss 0.11968371272087097 \n",
      "iterator 7390 : loss 0.11811307817697525 \n",
      "iterator 7400 : loss 0.12013664841651917 \n",
      "iterator 7410 : loss 0.12679477035999298 \n",
      "iterator 7420 : loss 0.11313702911138535 \n",
      "iterator 7430 : loss 0.11551673710346222 \n",
      "iterator 7440 : loss 0.11678257584571838 \n",
      "iterator 7450 : loss 0.11591094732284546 \n",
      "iterator 7460 : loss 0.10925965756177902 \n",
      "iterator 7470 : loss 0.1240454688668251 \n",
      "iterator 7480 : loss 0.11017360538244247 \n",
      "iterator 7490 : loss 0.11613725870847702 \n",
      "iterator 7500 : loss 0.11503349244594574 \n",
      "iterator 7510 : loss 0.12072201818227768 \n",
      "iterator 7520 : loss 0.11947001516819 \n",
      "iterator 7530 : loss 0.1181565672159195 \n",
      "iterator 7540 : loss 0.11729835718870163 \n",
      "iterator 7550 : loss 0.11606954783201218 \n",
      "iterator 7560 : loss 0.11425787210464478 \n",
      "iterator 7570 : loss 0.12023276090621948 \n",
      "iterator 7580 : loss 0.1271274983882904 \n",
      "iterator 7590 : loss 0.1089114397764206 \n",
      "iterator 7600 : loss 0.12431000918149948 \n",
      "iterator 7610 : loss 0.12208212912082672 \n",
      "iterator 7620 : loss 0.12190577387809753 \n",
      "iterator 7630 : loss 0.12348992377519608 \n",
      "iterator 7640 : loss 0.1132102683186531 \n",
      "iterator 7650 : loss 0.1186467856168747 \n",
      "iterator 7660 : loss 0.11777275800704956 \n",
      "iterator 7670 : loss 0.12042254209518433 \n",
      "iterator 7680 : loss 0.11224308609962463 \n",
      "iterator 7690 : loss 0.1153121292591095 \n",
      "iterator 7700 : loss 0.11144635081291199 \n",
      "iterator 7710 : loss 0.12306664139032364 \n",
      "iterator 7720 : loss 0.11670369654893875 \n",
      "iterator 7730 : loss 0.11153414100408554 \n",
      "iterator 7740 : loss 0.11664696782827377 \n",
      "iterator 7750 : loss 0.12766747176647186 \n",
      "iterator 7760 : loss 0.11377628147602081 \n",
      "iterator 7770 : loss 0.11058617383241653 \n",
      "iterator 7780 : loss 0.12593050301074982 \n",
      "iterator 7790 : loss 0.11547114700078964 \n",
      "iterator 7800 : loss 0.12692910432815552 \n",
      "iterator 7810 : loss 0.1185619980096817 \n",
      "iterator 7820 : loss 0.12050279974937439 \n",
      "iterator 7830 : loss 0.12016364187002182 \n",
      "iterator 7840 : loss 0.11100596189498901 \n",
      "iterator 7850 : loss 0.11555475741624832 \n",
      "iterator 7860 : loss 0.11667128652334213 \n",
      "iterator 7870 : loss 0.1105831116437912 \n",
      "iterator 7880 : loss 0.1247817873954773 \n",
      "iterator 7890 : loss 0.13210509717464447 \n",
      "iterator 7900 : loss 0.114948570728302 \n",
      "iterator 7910 : loss 0.11254501342773438 \n",
      "iterator 7920 : loss 0.1139867752790451 \n",
      "iterator 7930 : loss 0.10954096168279648 \n",
      "iterator 7940 : loss 0.11348520219326019 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 7950 : loss 0.12059485912322998 \n",
      "iterator 7960 : loss 0.1293175220489502 \n",
      "iterator 7970 : loss 0.1062617376446724 \n",
      "iterator 7980 : loss 0.12399356067180634 \n",
      "iterator 7990 : loss 0.11000274866819382 \n",
      "iterator 8000 : loss 0.11017569899559021 \n",
      "iterator 8010 : loss 0.11582321673631668 \n",
      "iterator 8020 : loss 0.11866522580385208 \n",
      "iterator 8030 : loss 0.11235446482896805 \n",
      "iterator 8040 : loss 0.11983906477689743 \n",
      "iterator 8050 : loss 0.11850149929523468 \n",
      "iterator 8060 : loss 0.11161037534475327 \n",
      "iterator 8070 : loss 0.12635715305805206 \n",
      "iterator 8080 : loss 0.12862083315849304 \n",
      "iterator 8090 : loss 0.12606582045555115 \n",
      "iterator 8100 : loss 0.13668064773082733 \n",
      "iterator 8110 : loss 0.12171943485736847 \n",
      "iterator 8120 : loss 0.13568682968616486 \n",
      "iterator 8130 : loss 0.11718255281448364 \n",
      "iterator 8140 : loss 0.1149691641330719 \n",
      "iterator 8150 : loss 0.11927048116922379 \n",
      "iterator 8160 : loss 0.12202285975217819 \n",
      "iterator 8170 : loss 0.10526252537965775 \n",
      "iterator 8180 : loss 0.1252126842737198 \n",
      "iterator 8190 : loss 0.1191631630063057 \n",
      "iterator 8200 : loss 0.11080728471279144 \n",
      "iterator 8210 : loss 0.11033535748720169 \n",
      "iterator 8220 : loss 0.11240450292825699 \n",
      "iterator 8230 : loss 0.1050710678100586 \n",
      "iterator 8240 : loss 0.11438979208469391 \n",
      "iterator 8250 : loss 0.120960533618927 \n",
      "iterator 8260 : loss 0.11510144174098969 \n",
      "iterator 8270 : loss 0.10839468240737915 \n",
      "iterator 8280 : loss 0.12933437526226044 \n",
      "iterator 8290 : loss 0.12240839749574661 \n",
      "iterator 8300 : loss 0.12357312440872192 \n",
      "iterator 8310 : loss 0.11551296710968018 \n",
      "iterator 8320 : loss 0.11819405108690262 \n",
      "iterator 8330 : loss 0.11743544787168503 \n",
      "iterator 8340 : loss 0.11422579735517502 \n",
      "iterator 8350 : loss 0.11786647886037827 \n",
      "iterator 8360 : loss 0.11896080523729324 \n",
      "iterator 8370 : loss 0.1169763132929802 \n",
      "iterator 8380 : loss 0.10713309794664383 \n",
      "iterator 8390 : loss 0.1100570410490036 \n",
      "iterator 8400 : loss 0.12049000710248947 \n",
      "iterator 8410 : loss 0.11687877029180527 \n",
      "iterator 8420 : loss 0.11721229553222656 \n",
      "iterator 8430 : loss 0.12837843596935272 \n",
      "iterator 8440 : loss 0.10434745997190475 \n",
      "iterator 8450 : loss 0.11915560811758041 \n",
      "iterator 8460 : loss 0.11371801048517227 \n",
      "iterator 8470 : loss 0.1152474507689476 \n",
      "iterator 8480 : loss 0.12139707058668137 \n",
      "iterator 8490 : loss 0.11877419054508209 \n",
      "iterator 8500 : loss 0.12033736705780029 \n",
      "iterator 8510 : loss 0.12333262711763382 \n",
      "iterator 8520 : loss 0.11123939603567123 \n",
      "iterator 8530 : loss 0.11829578876495361 \n",
      "iterator 8540 : loss 0.12252900749444962 \n",
      "iterator 8550 : loss 0.1359553039073944 \n",
      "iterator 8560 : loss 0.12345457077026367 \n",
      "iterator 8570 : loss 0.12011342495679855 \n",
      "iterator 8580 : loss 0.11938600242137909 \n",
      "iterator 8590 : loss 0.13087689876556396 \n",
      "iterator 8600 : loss 0.11345107853412628 \n",
      "iterator 8610 : loss 0.10801967978477478 \n",
      "iterator 8620 : loss 0.12404965609312057 \n",
      "iterator 8630 : loss 0.11144287884235382 \n",
      "iterator 8640 : loss 0.11314745992422104 \n",
      "iterator 8650 : loss 0.11361798644065857 \n",
      "iterator 8660 : loss 0.11648012697696686 \n",
      "iterator 8670 : loss 0.11188526451587677 \n",
      "iterator 8680 : loss 0.11877494305372238 \n",
      "iterator 8690 : loss 0.12251647561788559 \n",
      "iterator 8700 : loss 0.10674998164176941 \n",
      "iterator 8710 : loss 0.11246789246797562 \n",
      "iterator 8720 : loss 0.12404505163431168 \n",
      "iterator 8730 : loss 0.1166704073548317 \n",
      "iterator 8740 : loss 0.10698703676462173 \n",
      "iterator 8750 : loss 0.11314090341329575 \n",
      "iterator 8760 : loss 0.11187034100294113 \n",
      "iterator 8770 : loss 0.12360048294067383 \n",
      "iterator 8780 : loss 0.11650107800960541 \n",
      "iterator 8790 : loss 0.11089426279067993 \n",
      "iterator 8800 : loss 0.11355119943618774 \n",
      "iterator 8810 : loss 0.1193741112947464 \n",
      "iterator 8820 : loss 0.13026750087738037 \n",
      "iterator 8830 : loss 0.11515689641237259 \n",
      "iterator 8840 : loss 0.11239027231931686 \n",
      "iterator 8850 : loss 0.10826466232538223 \n",
      "iterator 8860 : loss 0.10632441192865372 \n",
      "iterator 8870 : loss 0.11327151954174042 \n",
      "iterator 8880 : loss 0.10803699493408203 \n",
      "iterator 8890 : loss 0.11565198749303818 \n",
      "iterator 8900 : loss 0.11487894505262375 \n",
      "iterator 8910 : loss 0.12006652355194092 \n",
      "iterator 8920 : loss 0.11976166814565659 \n",
      "iterator 8930 : loss 0.10252171754837036 \n",
      "iterator 8940 : loss 0.119440957903862 \n",
      "iterator 8950 : loss 0.10922577977180481 \n",
      "iterator 8960 : loss 0.10345416516065598 \n",
      "iterator 8970 : loss 0.11471176147460938 \n",
      "iterator 8980 : loss 0.1137775108218193 \n",
      "iterator 8990 : loss 0.11233688145875931 \n",
      "iterator 9000 : loss 0.11745516955852509 \n",
      "iterator 9010 : loss 0.11779657006263733 \n",
      "iterator 9020 : loss 0.11752471327781677 \n",
      "iterator 9030 : loss 0.1143135353922844 \n",
      "iterator 9040 : loss 0.11662450432777405 \n",
      "iterator 9050 : loss 0.11021645367145538 \n",
      "iterator 9060 : loss 0.12560005486011505 \n",
      "iterator 9070 : loss 0.12272819131612778 \n",
      "iterator 9080 : loss 0.12471910566091537 \n",
      "iterator 9090 : loss 0.11213666200637817 \n",
      "iterator 9100 : loss 0.11962870508432388 \n",
      "iterator 9110 : loss 0.11183811724185944 \n",
      "iterator 9120 : loss 0.11601879447698593 \n",
      "iterator 9130 : loss 0.12410678714513779 \n",
      "iterator 9140 : loss 0.11715015023946762 \n",
      "iterator 9150 : loss 0.1149713471531868 \n",
      "iterator 9160 : loss 0.11143184453248978 \n",
      "iterator 9170 : loss 0.10628204792737961 \n",
      "iterator 9180 : loss 0.1131182610988617 \n",
      "iterator 9190 : loss 0.12275314331054688 \n",
      "iterator 9200 : loss 0.11874177306890488 \n",
      "iterator 9210 : loss 0.11470942944288254 \n",
      "iterator 9220 : loss 0.11938348412513733 \n",
      "iterator 9230 : loss 0.10874856263399124 \n",
      "iterator 9240 : loss 0.11014385521411896 \n",
      "iterator 9250 : loss 0.11213503032922745 \n",
      "iterator 9260 : loss 0.11874795705080032 \n",
      "iterator 9270 : loss 0.10911336541175842 \n",
      "iterator 9280 : loss 0.11769908666610718 \n",
      "iterator 9290 : loss 0.11588071286678314 \n",
      "iterator 9300 : loss 0.11294664442539215 \n",
      "iterator 9310 : loss 0.10296566784381866 \n",
      "iterator 9320 : loss 0.10863945633172989 \n",
      "iterator 9330 : loss 0.1056089699268341 \n",
      "iterator 9340 : loss 0.11244861781597137 \n",
      "iterator 9350 : loss 0.10936197638511658 \n",
      "iterator 9360 : loss 0.10782425105571747 \n",
      "iterator 9370 : loss 0.11100047826766968 \n",
      "iterator 9380 : loss 0.1089123785495758 \n",
      "iterator 9390 : loss 0.11602524667978287 \n",
      "iterator 9400 : loss 0.10957875847816467 \n",
      "iterator 9410 : loss 0.11077248305082321 \n",
      "iterator 9420 : loss 0.11869177222251892 \n",
      "iterator 9430 : loss 0.10621559619903564 \n",
      "iterator 9440 : loss 0.11535581201314926 \n",
      "iterator 9450 : loss 0.11991026997566223 \n",
      "iterator 9460 : loss 0.11112753301858902 \n",
      "iterator 9470 : loss 0.12394285947084427 \n",
      "iterator 9480 : loss 0.10719209909439087 \n",
      "iterator 9490 : loss 0.11915262043476105 \n",
      "iterator 9500 : loss 0.10690256953239441 \n",
      "iterator 9510 : loss 0.10935411602258682 \n",
      "iterator 9520 : loss 0.10768506675958633 \n",
      "iterator 9530 : loss 0.11907146126031876 \n",
      "iterator 9540 : loss 0.11534563452005386 \n",
      "iterator 9550 : loss 0.12231022864580154 \n",
      "iterator 9560 : loss 0.1110464334487915 \n",
      "iterator 9570 : loss 0.11653432995080948 \n",
      "iterator 9580 : loss 0.10548371821641922 \n",
      "iterator 9590 : loss 0.10942046344280243 \n",
      "iterator 9600 : loss 0.12068828195333481 \n",
      "iterator 9610 : loss 0.11303135007619858 \n",
      "iterator 9620 : loss 0.1227535605430603 \n",
      "iterator 9630 : loss 0.11761315166950226 \n",
      "iterator 9640 : loss 0.10283441096544266 \n",
      "iterator 9650 : loss 0.10443200916051865 \n",
      "iterator 9660 : loss 0.11318683624267578 \n",
      "iterator 9670 : loss 0.11708981543779373 \n",
      "iterator 9680 : loss 0.11310072988271713 \n",
      "iterator 9690 : loss 0.10346152633428574 \n",
      "iterator 9700 : loss 0.10431520640850067 \n",
      "iterator 9710 : loss 0.10499852150678635 \n",
      "iterator 9720 : loss 0.1118789091706276 \n",
      "iterator 9730 : loss 0.11494985222816467 \n",
      "iterator 9740 : loss 0.11587892472743988 \n",
      "iterator 9750 : loss 0.11530209332704544 \n",
      "iterator 9760 : loss 0.12502042949199677 \n",
      "iterator 9770 : loss 0.10960273444652557 \n",
      "iterator 9780 : loss 0.11557543277740479 \n",
      "iterator 9790 : loss 0.10790002346038818 \n",
      "iterator 9800 : loss 0.11375291645526886 \n",
      "iterator 9810 : loss 0.11815526336431503 \n",
      "iterator 9820 : loss 0.104664646089077 \n",
      "iterator 9830 : loss 0.10903295874595642 \n",
      "iterator 9840 : loss 0.12120940536260605 \n",
      "iterator 9850 : loss 0.11659785360097885 \n",
      "iterator 9860 : loss 0.10525824129581451 \n",
      "iterator 9870 : loss 0.10932182520627975 \n",
      "iterator 9880 : loss 0.1166684702038765 \n",
      "iterator 9890 : loss 0.12208500504493713 \n",
      "iterator 9900 : loss 0.10819870978593826 \n",
      "iterator 9910 : loss 0.1107414960861206 \n",
      "iterator 9920 : loss 0.1318875551223755 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator 9930 : loss 0.10970941931009293 \n",
      "iterator 9940 : loss 0.11579509824514389 \n",
      "iterator 9950 : loss 0.12147434055805206 \n",
      "iterator 9960 : loss 0.10920371115207672 \n",
      "iterator 9970 : loss 0.10565339028835297 \n",
      "iterator 9980 : loss 0.10987193882465363 \n",
      "iterator 9990 : loss 0.10864824056625366 \n",
      "iterator 10000 : loss 0.111347995698452 \n"
     ]
    }
   ],
   "source": [
    "x.train(iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
